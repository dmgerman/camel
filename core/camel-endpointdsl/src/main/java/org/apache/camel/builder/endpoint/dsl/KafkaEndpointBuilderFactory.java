begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *      http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
DECL|package|org.apache.camel.builder.endpoint.dsl
package|package
name|org
operator|.
name|apache
operator|.
name|camel
operator|.
name|builder
operator|.
name|endpoint
operator|.
name|dsl
package|;
end_package

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutorService
import|;
end_import

begin_import
import|import
name|javax
operator|.
name|annotation
operator|.
name|Generated
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|camel
operator|.
name|ExchangePattern
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|camel
operator|.
name|builder
operator|.
name|EndpointConsumerBuilder
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|camel
operator|.
name|builder
operator|.
name|EndpointProducerBuilder
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|camel
operator|.
name|builder
operator|.
name|endpoint
operator|.
name|AbstractEndpointBuilder
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|camel
operator|.
name|spi
operator|.
name|ExceptionHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|camel
operator|.
name|spi
operator|.
name|HeaderFilterStrategy
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|camel
operator|.
name|spi
operator|.
name|StateRepository
import|;
end_import

begin_comment
comment|/**  * The kafka component allows messages to be sent to (or consumed from) Apache  * Kafka brokers.  *   * Generated by camel-package-maven-plugin - do not edit this file!  */
end_comment

begin_interface
annotation|@
name|Generated
argument_list|(
literal|"org.apache.camel.maven.packaging.EndpointDslMojo"
argument_list|)
DECL|interface|KafkaEndpointBuilderFactory
specifier|public
interface|interface
name|KafkaEndpointBuilderFactory
block|{
comment|/**      * Builder for endpoint consumers for the Kafka component.      */
DECL|interface|KafkaEndpointConsumerBuilder
specifier|public
interface|interface
name|KafkaEndpointConsumerBuilder
extends|extends
name|EndpointConsumerBuilder
block|{
DECL|method|advanced ()
specifier|default
name|AdvancedKafkaEndpointConsumerBuilder
name|advanced
parameter_list|()
block|{
return|return
operator|(
name|AdvancedKafkaEndpointConsumerBuilder
operator|)
name|this
return|;
block|}
comment|/**          * URL of the Kafka brokers to use. The format is          * host1:port1,host2:port2, and the list can be a subset of brokers or a          * VIP pointing to a subset of brokers. This option is known as          * bootstrap.servers in the Kafka documentation.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: common          */
DECL|method|brokers (String brokers)
specifier|default
name|KafkaEndpointConsumerBuilder
name|brokers
parameter_list|(
name|String
name|brokers
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"brokers"
argument_list|,
name|brokers
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The client id is a user-specified string sent in each request to help          * trace calls. It should logically identify the application making the          * request.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: common          */
DECL|method|clientId (String clientId)
specifier|default
name|KafkaEndpointConsumerBuilder
name|clientId
parameter_list|(
name|String
name|clientId
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"clientId"
argument_list|,
name|clientId
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * To use a custom HeaderFilterStrategy to filter header to and from          * Camel message.          *           * The option is a:          *<code>org.apache.camel.spi.HeaderFilterStrategy</code> type.          *           * Group: common          */
DECL|method|headerFilterStrategy ( HeaderFilterStrategy headerFilterStrategy)
specifier|default
name|KafkaEndpointConsumerBuilder
name|headerFilterStrategy
parameter_list|(
name|HeaderFilterStrategy
name|headerFilterStrategy
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"headerFilterStrategy"
argument_list|,
name|headerFilterStrategy
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * To use a custom HeaderFilterStrategy to filter header to and from          * Camel message.          *           * The option will be converted to a          *<code>org.apache.camel.spi.HeaderFilterStrategy</code> type.          *           * Group: common          */
DECL|method|headerFilterStrategy ( String headerFilterStrategy)
specifier|default
name|KafkaEndpointConsumerBuilder
name|headerFilterStrategy
parameter_list|(
name|String
name|headerFilterStrategy
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"headerFilterStrategy"
argument_list|,
name|headerFilterStrategy
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of time in milliseconds to wait when reconnecting          * to a broker that has repeatedly failed to connect. If provided, the          * backoff per host will increase exponentially for each consecutive          * connection failure, up to this maximum. After calculating the backoff          * increase, 20% random jitter is added to avoid connection storms.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: common          */
DECL|method|reconnectBackoffMaxMs ( Integer reconnectBackoffMaxMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|reconnectBackoffMaxMs
parameter_list|(
name|Integer
name|reconnectBackoffMaxMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"reconnectBackoffMaxMs"
argument_list|,
name|reconnectBackoffMaxMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of time in milliseconds to wait when reconnecting          * to a broker that has repeatedly failed to connect. If provided, the          * backoff per host will increase exponentially for each consecutive          * connection failure, up to this maximum. After calculating the backoff          * increase, 20% random jitter is added to avoid connection storms.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: common          */
DECL|method|reconnectBackoffMaxMs ( String reconnectBackoffMaxMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|reconnectBackoffMaxMs
parameter_list|(
name|String
name|reconnectBackoffMaxMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"reconnectBackoffMaxMs"
argument_list|,
name|reconnectBackoffMaxMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether to allow doing manual commits via KafkaManualCommit. If this          * option is enabled then an instance of KafkaManualCommit is stored on          * the Exchange message header, which allows end users to access this          * API and perform manual offset commits via the Kafka consumer.          *           * The option is a:<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|allowManualCommit ( boolean allowManualCommit)
specifier|default
name|KafkaEndpointConsumerBuilder
name|allowManualCommit
parameter_list|(
name|boolean
name|allowManualCommit
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"allowManualCommit"
argument_list|,
name|allowManualCommit
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether to allow doing manual commits via KafkaManualCommit. If this          * option is enabled then an instance of KafkaManualCommit is stored on          * the Exchange message header, which allows end users to access this          * API and perform manual offset commits via the Kafka consumer.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|allowManualCommit ( String allowManualCommit)
specifier|default
name|KafkaEndpointConsumerBuilder
name|allowManualCommit
parameter_list|(
name|String
name|allowManualCommit
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"allowManualCommit"
argument_list|,
name|allowManualCommit
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * If true, periodically commit to ZooKeeper the offset of messages          * already fetched by the consumer. This committed offset will be used          * when the process fails as the position from which the new consumer          * will begin.          *           * The option is a:<code>java.lang.Boolean</code> type.          *           * Group: consumer          */
DECL|method|autoCommitEnable ( Boolean autoCommitEnable)
specifier|default
name|KafkaEndpointConsumerBuilder
name|autoCommitEnable
parameter_list|(
name|Boolean
name|autoCommitEnable
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"autoCommitEnable"
argument_list|,
name|autoCommitEnable
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * If true, periodically commit to ZooKeeper the offset of messages          * already fetched by the consumer. This committed offset will be used          * when the process fails as the position from which the new consumer          * will begin.          *           * The option will be converted to a<code>java.lang.Boolean</code>          * type.          *           * Group: consumer          */
DECL|method|autoCommitEnable ( String autoCommitEnable)
specifier|default
name|KafkaEndpointConsumerBuilder
name|autoCommitEnable
parameter_list|(
name|String
name|autoCommitEnable
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"autoCommitEnable"
argument_list|,
name|autoCommitEnable
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The frequency in ms that the consumer offsets are committed to          * zookeeper.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: consumer          */
DECL|method|autoCommitIntervalMs ( Integer autoCommitIntervalMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|autoCommitIntervalMs
parameter_list|(
name|Integer
name|autoCommitIntervalMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"autoCommitIntervalMs"
argument_list|,
name|autoCommitIntervalMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The frequency in ms that the consumer offsets are committed to          * zookeeper.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: consumer          */
DECL|method|autoCommitIntervalMs ( String autoCommitIntervalMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|autoCommitIntervalMs
parameter_list|(
name|String
name|autoCommitIntervalMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"autoCommitIntervalMs"
argument_list|,
name|autoCommitIntervalMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether to perform an explicit auto commit when the consumer stops to          * ensure the broker has a commit from the last consumed message. This          * requires the option autoCommitEnable is turned on. The possible          * values are: sync, async, or none. And sync is the default value.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: consumer          */
DECL|method|autoCommitOnStop ( String autoCommitOnStop)
specifier|default
name|KafkaEndpointConsumerBuilder
name|autoCommitOnStop
parameter_list|(
name|String
name|autoCommitOnStop
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"autoCommitOnStop"
argument_list|,
name|autoCommitOnStop
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * What to do when there is no initial offset in ZooKeeper or if an          * offset is out of range: earliest : automatically reset the offset to          * the earliest offset latest : automatically reset the offset to the          * latest offset fail: throw exception to the consumer.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: consumer          */
DECL|method|autoOffsetReset ( String autoOffsetReset)
specifier|default
name|KafkaEndpointConsumerBuilder
name|autoOffsetReset
parameter_list|(
name|String
name|autoOffsetReset
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"autoOffsetReset"
argument_list|,
name|autoOffsetReset
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * This options controls what happens when a consumer is processing an          * exchange and it fails. If the option is false then the consumer          * continues to the next message and processes it. If the option is true          * then the consumer breaks out, and will seek back to offset of the          * message that caused a failure, and then re-attempt to process this          * message. However this can lead to endless processing of the same          * message if its bound to fail every time, eg a poison message.          * Therefore its recommended to deal with that for example by using          * Camel's error handler.          *           * The option is a:<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|breakOnFirstError ( boolean breakOnFirstError)
specifier|default
name|KafkaEndpointConsumerBuilder
name|breakOnFirstError
parameter_list|(
name|boolean
name|breakOnFirstError
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"breakOnFirstError"
argument_list|,
name|breakOnFirstError
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * This options controls what happens when a consumer is processing an          * exchange and it fails. If the option is false then the consumer          * continues to the next message and processes it. If the option is true          * then the consumer breaks out, and will seek back to offset of the          * message that caused a failure, and then re-attempt to process this          * message. However this can lead to endless processing of the same          * message if its bound to fail every time, eg a poison message.          * Therefore its recommended to deal with that for example by using          * Camel's error handler.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|breakOnFirstError ( String breakOnFirstError)
specifier|default
name|KafkaEndpointConsumerBuilder
name|breakOnFirstError
parameter_list|(
name|String
name|breakOnFirstError
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"breakOnFirstError"
argument_list|,
name|breakOnFirstError
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Allows for bridging the consumer to the Camel routing Error Handler,          * which mean any exceptions occurred while the consumer is trying to          * pickup incoming messages, or the likes, will now be processed as a          * message and handled by the routing Error Handler. By default the          * consumer will use the org.apache.camel.spi.ExceptionHandler to deal          * with exceptions, that will be logged at WARN or ERROR level and          * ignored.          *           * The option is a:<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|bridgeErrorHandler ( boolean bridgeErrorHandler)
specifier|default
name|KafkaEndpointConsumerBuilder
name|bridgeErrorHandler
parameter_list|(
name|boolean
name|bridgeErrorHandler
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"bridgeErrorHandler"
argument_list|,
name|bridgeErrorHandler
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Allows for bridging the consumer to the Camel routing Error Handler,          * which mean any exceptions occurred while the consumer is trying to          * pickup incoming messages, or the likes, will now be processed as a          * message and handled by the routing Error Handler. By default the          * consumer will use the org.apache.camel.spi.ExceptionHandler to deal          * with exceptions, that will be logged at WARN or ERROR level and          * ignored.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|bridgeErrorHandler ( String bridgeErrorHandler)
specifier|default
name|KafkaEndpointConsumerBuilder
name|bridgeErrorHandler
parameter_list|(
name|String
name|bridgeErrorHandler
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"bridgeErrorHandler"
argument_list|,
name|bridgeErrorHandler
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Automatically check the CRC32 of the records consumed. This ensures          * no on-the-wire or on-disk corruption to the messages occurred. This          * check adds some overhead, so it may be disabled in cases seeking          * extreme performance.          *           * The option is a:<code>java.lang.Boolean</code> type.          *           * Group: consumer          */
DECL|method|checkCrcs (Boolean checkCrcs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|checkCrcs
parameter_list|(
name|Boolean
name|checkCrcs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"checkCrcs"
argument_list|,
name|checkCrcs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Automatically check the CRC32 of the records consumed. This ensures          * no on-the-wire or on-disk corruption to the messages occurred. This          * check adds some overhead, so it may be disabled in cases seeking          * extreme performance.          *           * The option will be converted to a<code>java.lang.Boolean</code>          * type.          *           * Group: consumer          */
DECL|method|checkCrcs (String checkCrcs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|checkCrcs
parameter_list|(
name|String
name|checkCrcs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"checkCrcs"
argument_list|,
name|checkCrcs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The configuration controls the maximum amount of time the client will          * wait for the response of a request. If the response is not received          * before the timeout elapses the client will resend the request if          * necessary or fail the request if retries are exhausted.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: consumer          */
DECL|method|consumerRequestTimeoutMs ( Integer consumerRequestTimeoutMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|consumerRequestTimeoutMs
parameter_list|(
name|Integer
name|consumerRequestTimeoutMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"consumerRequestTimeoutMs"
argument_list|,
name|consumerRequestTimeoutMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The configuration controls the maximum amount of time the client will          * wait for the response of a request. If the response is not received          * before the timeout elapses the client will resend the request if          * necessary or fail the request if retries are exhausted.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: consumer          */
DECL|method|consumerRequestTimeoutMs ( String consumerRequestTimeoutMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|consumerRequestTimeoutMs
parameter_list|(
name|String
name|consumerRequestTimeoutMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"consumerRequestTimeoutMs"
argument_list|,
name|consumerRequestTimeoutMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The number of consumers that connect to kafka server.          *           * The option is a:<code>int</code> type.          *           * Group: consumer          */
DECL|method|consumersCount (int consumersCount)
specifier|default
name|KafkaEndpointConsumerBuilder
name|consumersCount
parameter_list|(
name|int
name|consumersCount
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"consumersCount"
argument_list|,
name|consumersCount
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The number of consumers that connect to kafka server.          *           * The option will be converted to a<code>int</code> type.          *           * Group: consumer          */
DECL|method|consumersCount ( String consumersCount)
specifier|default
name|KafkaEndpointConsumerBuilder
name|consumersCount
parameter_list|(
name|String
name|consumersCount
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"consumersCount"
argument_list|,
name|consumersCount
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Number of concurrent consumers on the consumer.          *           * The option is a:<code>int</code> type.          *           * Group: consumer          */
DECL|method|consumerStreams (int consumerStreams)
specifier|default
name|KafkaEndpointConsumerBuilder
name|consumerStreams
parameter_list|(
name|int
name|consumerStreams
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"consumerStreams"
argument_list|,
name|consumerStreams
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Number of concurrent consumers on the consumer.          *           * The option will be converted to a<code>int</code> type.          *           * Group: consumer          */
DECL|method|consumerStreams ( String consumerStreams)
specifier|default
name|KafkaEndpointConsumerBuilder
name|consumerStreams
parameter_list|(
name|String
name|consumerStreams
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"consumerStreams"
argument_list|,
name|consumerStreams
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of data the server should return for a fetch          * request This is not an absolute maximum, if the first message in the          * first non-empty partition of the fetch is larger than this value, the          * message will still be returned to ensure that the consumer can make          * progress. The maximum message size accepted by the broker is defined          * via message.max.bytes (broker config) or max.message.bytes (topic          * config). Note that the consumer performs multiple fetches in          * parallel.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: consumer          */
DECL|method|fetchMaxBytes (Integer fetchMaxBytes)
specifier|default
name|KafkaEndpointConsumerBuilder
name|fetchMaxBytes
parameter_list|(
name|Integer
name|fetchMaxBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"fetchMaxBytes"
argument_list|,
name|fetchMaxBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of data the server should return for a fetch          * request This is not an absolute maximum, if the first message in the          * first non-empty partition of the fetch is larger than this value, the          * message will still be returned to ensure that the consumer can make          * progress. The maximum message size accepted by the broker is defined          * via message.max.bytes (broker config) or max.message.bytes (topic          * config). Note that the consumer performs multiple fetches in          * parallel.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: consumer          */
DECL|method|fetchMaxBytes (String fetchMaxBytes)
specifier|default
name|KafkaEndpointConsumerBuilder
name|fetchMaxBytes
parameter_list|(
name|String
name|fetchMaxBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"fetchMaxBytes"
argument_list|,
name|fetchMaxBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The minimum amount of data the server should return for a fetch          * request. If insufficient data is available the request will wait for          * that much data to accumulate before answering the request.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: consumer          */
DECL|method|fetchMinBytes (Integer fetchMinBytes)
specifier|default
name|KafkaEndpointConsumerBuilder
name|fetchMinBytes
parameter_list|(
name|Integer
name|fetchMinBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"fetchMinBytes"
argument_list|,
name|fetchMinBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The minimum amount of data the server should return for a fetch          * request. If insufficient data is available the request will wait for          * that much data to accumulate before answering the request.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: consumer          */
DECL|method|fetchMinBytes (String fetchMinBytes)
specifier|default
name|KafkaEndpointConsumerBuilder
name|fetchMinBytes
parameter_list|(
name|String
name|fetchMinBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"fetchMinBytes"
argument_list|,
name|fetchMinBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of time the server will block before answering the          * fetch request if there isn't sufficient data to immediately satisfy          * fetch.min.bytes.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: consumer          */
DECL|method|fetchWaitMaxMs ( Integer fetchWaitMaxMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|fetchWaitMaxMs
parameter_list|(
name|Integer
name|fetchWaitMaxMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"fetchWaitMaxMs"
argument_list|,
name|fetchWaitMaxMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of time the server will block before answering the          * fetch request if there isn't sufficient data to immediately satisfy          * fetch.min.bytes.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: consumer          */
DECL|method|fetchWaitMaxMs ( String fetchWaitMaxMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|fetchWaitMaxMs
parameter_list|(
name|String
name|fetchWaitMaxMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"fetchWaitMaxMs"
argument_list|,
name|fetchWaitMaxMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * A string that uniquely identifies the group of consumer processes to          * which this consumer belongs. By setting the same group id multiple          * processes indicate that they are all part of the same consumer group.          * This option is required for consumers.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: consumer          */
DECL|method|groupId (String groupId)
specifier|default
name|KafkaEndpointConsumerBuilder
name|groupId
parameter_list|(
name|String
name|groupId
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"groupId"
argument_list|,
name|groupId
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The expected time between heartbeats to the consumer coordinator when          * using Kafka's group management facilities. Heartbeats are used to          * ensure that the consumer's session stays active and to facilitate          * rebalancing when new consumers join or leave the group. The value          * must be set lower than session.timeout.ms, but typically should be          * set no higher than 1/3 of that value. It can be adjusted even lower          * to control the expected time for normal rebalances.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: consumer          */
DECL|method|heartbeatIntervalMs ( Integer heartbeatIntervalMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|heartbeatIntervalMs
parameter_list|(
name|Integer
name|heartbeatIntervalMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"heartbeatIntervalMs"
argument_list|,
name|heartbeatIntervalMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The expected time between heartbeats to the consumer coordinator when          * using Kafka's group management facilities. Heartbeats are used to          * ensure that the consumer's session stays active and to facilitate          * rebalancing when new consumers join or leave the group. The value          * must be set lower than session.timeout.ms, but typically should be          * set no higher than 1/3 of that value. It can be adjusted even lower          * to control the expected time for normal rebalances.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: consumer          */
DECL|method|heartbeatIntervalMs ( String heartbeatIntervalMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|heartbeatIntervalMs
parameter_list|(
name|String
name|heartbeatIntervalMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"heartbeatIntervalMs"
argument_list|,
name|heartbeatIntervalMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets custom KafkaHeaderDeserializer for deserialization kafka headers          * values to camel headers values.          *           * The option is a:          *<code>org.apache.camel.component.kafka.serde.KafkaHeaderDeserializer</code> type.          *           * Group: consumer          */
DECL|method|kafkaHeaderDeserializer ( Object kafkaHeaderDeserializer)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kafkaHeaderDeserializer
parameter_list|(
name|Object
name|kafkaHeaderDeserializer
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kafkaHeaderDeserializer"
argument_list|,
name|kafkaHeaderDeserializer
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets custom KafkaHeaderDeserializer for deserialization kafka headers          * values to camel headers values.          *           * The option will be converted to a          *<code>org.apache.camel.component.kafka.serde.KafkaHeaderDeserializer</code> type.          *           * Group: consumer          */
DECL|method|kafkaHeaderDeserializer ( String kafkaHeaderDeserializer)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kafkaHeaderDeserializer
parameter_list|(
name|String
name|kafkaHeaderDeserializer
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kafkaHeaderDeserializer"
argument_list|,
name|kafkaHeaderDeserializer
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Deserializer class for key that implements the Deserializer          * interface.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: consumer          */
DECL|method|keyDeserializer ( String keyDeserializer)
specifier|default
name|KafkaEndpointConsumerBuilder
name|keyDeserializer
parameter_list|(
name|String
name|keyDeserializer
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"keyDeserializer"
argument_list|,
name|keyDeserializer
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of data per-partition the server will return. The          * maximum total memory used for a request will be #partitions          * max.partition.fetch.bytes. This size must be at least as large as the          * maximum message size the server allows or else it is possible for the          * producer to send messages larger than the consumer can fetch. If that          * happens, the consumer can get stuck trying to fetch a large message          * on a certain partition.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: consumer          */
DECL|method|maxPartitionFetchBytes ( Integer maxPartitionFetchBytes)
specifier|default
name|KafkaEndpointConsumerBuilder
name|maxPartitionFetchBytes
parameter_list|(
name|Integer
name|maxPartitionFetchBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxPartitionFetchBytes"
argument_list|,
name|maxPartitionFetchBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of data per-partition the server will return. The          * maximum total memory used for a request will be #partitions          * max.partition.fetch.bytes. This size must be at least as large as the          * maximum message size the server allows or else it is possible for the          * producer to send messages larger than the consumer can fetch. If that          * happens, the consumer can get stuck trying to fetch a large message          * on a certain partition.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: consumer          */
DECL|method|maxPartitionFetchBytes ( String maxPartitionFetchBytes)
specifier|default
name|KafkaEndpointConsumerBuilder
name|maxPartitionFetchBytes
parameter_list|(
name|String
name|maxPartitionFetchBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxPartitionFetchBytes"
argument_list|,
name|maxPartitionFetchBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum delay between invocations of poll() when using consumer          * group management. This places an upper bound on the amount of time          * that the consumer can be idle before fetching more records. If poll()          * is not called before expiration of this timeout, then the consumer is          * considered failed and the group will rebalance in order to reassign          * the partitions to another member.          *           * The option is a:<code>java.lang.Long</code> type.          *           * Group: consumer          */
DECL|method|maxPollIntervalMs ( Long maxPollIntervalMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|maxPollIntervalMs
parameter_list|(
name|Long
name|maxPollIntervalMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxPollIntervalMs"
argument_list|,
name|maxPollIntervalMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum delay between invocations of poll() when using consumer          * group management. This places an upper bound on the amount of time          * that the consumer can be idle before fetching more records. If poll()          * is not called before expiration of this timeout, then the consumer is          * considered failed and the group will rebalance in order to reassign          * the partitions to another member.          *           * The option will be converted to a<code>java.lang.Long</code> type.          *           * Group: consumer          */
DECL|method|maxPollIntervalMs ( String maxPollIntervalMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|maxPollIntervalMs
parameter_list|(
name|String
name|maxPollIntervalMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxPollIntervalMs"
argument_list|,
name|maxPollIntervalMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum number of records returned in a single call to poll().          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: consumer          */
DECL|method|maxPollRecords ( Integer maxPollRecords)
specifier|default
name|KafkaEndpointConsumerBuilder
name|maxPollRecords
parameter_list|(
name|Integer
name|maxPollRecords
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxPollRecords"
argument_list|,
name|maxPollRecords
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum number of records returned in a single call to poll().          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: consumer          */
DECL|method|maxPollRecords ( String maxPollRecords)
specifier|default
name|KafkaEndpointConsumerBuilder
name|maxPollRecords
parameter_list|(
name|String
name|maxPollRecords
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxPollRecords"
argument_list|,
name|maxPollRecords
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The offset repository to use in order to locally store the offset of          * each partition of the topic. Defining one will disable the          * autocommit.          *           * The option is a:          *<code>org.apache.camel.spi.StateRepository&lt;java.lang.String,          * java.lang.String&gt;</code> type.          *           * Group: consumer          */
DECL|method|offsetRepository ( StateRepository<String, String> offsetRepository)
specifier|default
name|KafkaEndpointConsumerBuilder
name|offsetRepository
parameter_list|(
name|StateRepository
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|offsetRepository
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"offsetRepository"
argument_list|,
name|offsetRepository
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The offset repository to use in order to locally store the offset of          * each partition of the topic. Defining one will disable the          * autocommit.          *           * The option will be converted to a          *<code>org.apache.camel.spi.StateRepository&lt;java.lang.String,          * java.lang.String&gt;</code> type.          *           * Group: consumer          */
DECL|method|offsetRepository ( String offsetRepository)
specifier|default
name|KafkaEndpointConsumerBuilder
name|offsetRepository
parameter_list|(
name|String
name|offsetRepository
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"offsetRepository"
argument_list|,
name|offsetRepository
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The class name of the partition assignment strategy that the client          * will use to distribute partition ownership amongst consumer instances          * when group management is used.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: consumer          */
DECL|method|partitionAssignor ( String partitionAssignor)
specifier|default
name|KafkaEndpointConsumerBuilder
name|partitionAssignor
parameter_list|(
name|String
name|partitionAssignor
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"partitionAssignor"
argument_list|,
name|partitionAssignor
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The timeout used when polling the KafkaConsumer.          *           * The option is a:<code>java.lang.Long</code> type.          *           * Group: consumer          */
DECL|method|pollTimeoutMs (Long pollTimeoutMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|pollTimeoutMs
parameter_list|(
name|Long
name|pollTimeoutMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"pollTimeoutMs"
argument_list|,
name|pollTimeoutMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The timeout used when polling the KafkaConsumer.          *           * The option will be converted to a<code>java.lang.Long</code> type.          *           * Group: consumer          */
DECL|method|pollTimeoutMs (String pollTimeoutMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|pollTimeoutMs
parameter_list|(
name|String
name|pollTimeoutMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"pollTimeoutMs"
argument_list|,
name|pollTimeoutMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Set if KafkaConsumer will read from beginning or end on startup:          * beginning : read from beginning end : read from end This is replacing          * the earlier property seekToBeginning.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: consumer          */
DECL|method|seekTo (String seekTo)
specifier|default
name|KafkaEndpointConsumerBuilder
name|seekTo
parameter_list|(
name|String
name|seekTo
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"seekTo"
argument_list|,
name|seekTo
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The timeout used to detect failures when using Kafka's group          * management facilities.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: consumer          */
DECL|method|sessionTimeoutMs ( Integer sessionTimeoutMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sessionTimeoutMs
parameter_list|(
name|Integer
name|sessionTimeoutMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sessionTimeoutMs"
argument_list|,
name|sessionTimeoutMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The timeout used to detect failures when using Kafka's group          * management facilities.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: consumer          */
DECL|method|sessionTimeoutMs ( String sessionTimeoutMs)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sessionTimeoutMs
parameter_list|(
name|String
name|sessionTimeoutMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sessionTimeoutMs"
argument_list|,
name|sessionTimeoutMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * This enables the use of a specific Avro reader for use with the          * Confluent Platform schema registry and the          * io.confluent.kafka.serializers.KafkaAvroDeserializer. This option is          * only available in the Confluent Platform (not standard Apache Kafka).          *           * The option is a:<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|specificAvroReader ( boolean specificAvroReader)
specifier|default
name|KafkaEndpointConsumerBuilder
name|specificAvroReader
parameter_list|(
name|boolean
name|specificAvroReader
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"specificAvroReader"
argument_list|,
name|specificAvroReader
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * This enables the use of a specific Avro reader for use with the          * Confluent Platform schema registry and the          * io.confluent.kafka.serializers.KafkaAvroDeserializer. This option is          * only available in the Confluent Platform (not standard Apache Kafka).          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|specificAvroReader ( String specificAvroReader)
specifier|default
name|KafkaEndpointConsumerBuilder
name|specificAvroReader
parameter_list|(
name|String
name|specificAvroReader
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"specificAvroReader"
argument_list|,
name|specificAvroReader
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the topic is a pattern (regular expression). This can be used          * to subscribe to dynamic number of topics matching the pattern.          *           * The option is a:<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|topicIsPattern ( boolean topicIsPattern)
specifier|default
name|KafkaEndpointConsumerBuilder
name|topicIsPattern
parameter_list|(
name|boolean
name|topicIsPattern
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"topicIsPattern"
argument_list|,
name|topicIsPattern
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the topic is a pattern (regular expression). This can be used          * to subscribe to dynamic number of topics matching the pattern.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: consumer          */
DECL|method|topicIsPattern ( String topicIsPattern)
specifier|default
name|KafkaEndpointConsumerBuilder
name|topicIsPattern
parameter_list|(
name|String
name|topicIsPattern
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"topicIsPattern"
argument_list|,
name|topicIsPattern
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Deserializer class for value that implements the Deserializer          * interface.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: consumer          */
DECL|method|valueDeserializer ( String valueDeserializer)
specifier|default
name|KafkaEndpointConsumerBuilder
name|valueDeserializer
parameter_list|(
name|String
name|valueDeserializer
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"valueDeserializer"
argument_list|,
name|valueDeserializer
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets interceptors for producer or consumers. Producer interceptors          * have to be classes implementing          * org.apache.kafka.clients.producer.ProducerInterceptor Consumer          * interceptors have to be classes implementing          * org.apache.kafka.clients.consumer.ConsumerInterceptor Note that if          * you use Producer interceptor on a consumer it will throw a class cast          * exception in runtime.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: monitoring          */
DECL|method|interceptorClasses ( String interceptorClasses)
specifier|default
name|KafkaEndpointConsumerBuilder
name|interceptorClasses
parameter_list|(
name|String
name|interceptorClasses
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"interceptorClasses"
argument_list|,
name|interceptorClasses
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread sleep time between refresh attempts.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: security          */
DECL|method|kerberosBeforeReloginMinTime ( Integer kerberosBeforeReloginMinTime)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kerberosBeforeReloginMinTime
parameter_list|(
name|Integer
name|kerberosBeforeReloginMinTime
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosBeforeReloginMinTime"
argument_list|,
name|kerberosBeforeReloginMinTime
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread sleep time between refresh attempts.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: security          */
DECL|method|kerberosBeforeReloginMinTime ( String kerberosBeforeReloginMinTime)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kerberosBeforeReloginMinTime
parameter_list|(
name|String
name|kerberosBeforeReloginMinTime
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosBeforeReloginMinTime"
argument_list|,
name|kerberosBeforeReloginMinTime
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Kerberos kinit command path. Default is /usr/bin/kinit.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|kerberosInitCmd ( String kerberosInitCmd)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kerberosInitCmd
parameter_list|(
name|String
name|kerberosInitCmd
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosInitCmd"
argument_list|,
name|kerberosInitCmd
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * A list of rules for mapping from principal names to short names          * (typically operating system usernames). The rules are evaluated in          * order and the first rule that matches a principal name is used to map          * it to a short name. Any later rules in the list are ignored. By          * default, principal names of the form {username}/{hostname}{REALM} are          * mapped to {username}. For more details on the format please see the          * security authorization and acls documentation.. Multiple values can          * be separated by comma.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|kerberosPrincipalToLocalRules ( String kerberosPrincipalToLocalRules)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kerberosPrincipalToLocalRules
parameter_list|(
name|String
name|kerberosPrincipalToLocalRules
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosPrincipalToLocalRules"
argument_list|,
name|kerberosPrincipalToLocalRules
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Percentage of random jitter added to the renewal time.          *           * The option is a:<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewJitter ( Double kerberosRenewJitter)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kerberosRenewJitter
parameter_list|(
name|Double
name|kerberosRenewJitter
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewJitter"
argument_list|,
name|kerberosRenewJitter
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Percentage of random jitter added to the renewal time.          *           * The option will be converted to a<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewJitter ( String kerberosRenewJitter)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kerberosRenewJitter
parameter_list|(
name|String
name|kerberosRenewJitter
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewJitter"
argument_list|,
name|kerberosRenewJitter
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread will sleep until the specified window factor of time          * from last refresh to ticket's expiry has been reached, at which time          * it will try to renew the ticket.          *           * The option is a:<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewWindowFactor ( Double kerberosRenewWindowFactor)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kerberosRenewWindowFactor
parameter_list|(
name|Double
name|kerberosRenewWindowFactor
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewWindowFactor"
argument_list|,
name|kerberosRenewWindowFactor
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread will sleep until the specified window factor of time          * from last refresh to ticket's expiry has been reached, at which time          * it will try to renew the ticket.          *           * The option will be converted to a<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewWindowFactor ( String kerberosRenewWindowFactor)
specifier|default
name|KafkaEndpointConsumerBuilder
name|kerberosRenewWindowFactor
parameter_list|(
name|String
name|kerberosRenewWindowFactor
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewWindowFactor"
argument_list|,
name|kerberosRenewWindowFactor
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Expose the kafka sasl.jaas.config parameter Example:          * org.apache.kafka.common.security.plain.PlainLoginModule required          * username=USERNAME password=PASSWORD;.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|saslJaasConfig ( String saslJaasConfig)
specifier|default
name|KafkaEndpointConsumerBuilder
name|saslJaasConfig
parameter_list|(
name|String
name|saslJaasConfig
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"saslJaasConfig"
argument_list|,
name|saslJaasConfig
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The Kerberos principal name that Kafka runs as. This can be defined          * either in Kafka's JAAS config or in Kafka's config.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|saslKerberosServiceName ( String saslKerberosServiceName)
specifier|default
name|KafkaEndpointConsumerBuilder
name|saslKerberosServiceName
parameter_list|(
name|String
name|saslKerberosServiceName
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"saslKerberosServiceName"
argument_list|,
name|saslKerberosServiceName
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The Simple Authentication and Security Layer (SASL) Mechanism used.          * For the valid values see a href=          * http://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtmlhttp://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|saslMechanism (String saslMechanism)
specifier|default
name|KafkaEndpointConsumerBuilder
name|saslMechanism
parameter_list|(
name|String
name|saslMechanism
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"saslMechanism"
argument_list|,
name|saslMechanism
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Protocol used to communicate with brokers. SASL_PLAINTEXT, PLAINTEXT          * and SSL are supported.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|securityProtocol ( String securityProtocol)
specifier|default
name|KafkaEndpointConsumerBuilder
name|securityProtocol
parameter_list|(
name|String
name|securityProtocol
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"securityProtocol"
argument_list|,
name|securityProtocol
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * A list of cipher suites. This is a named combination of          * authentication, encryption, MAC and key exchange algorithm used to          * negotiate the security settings for a network connection using TLS or          * SSL network protocol.By default all the available cipher suites are          * supported.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslCipherSuites ( String sslCipherSuites)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslCipherSuites
parameter_list|(
name|String
name|sslCipherSuites
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslCipherSuites"
argument_list|,
name|sslCipherSuites
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * SSL configuration using a Camel SSLContextParameters object. If          * configured it's applied before the other SSL endpoint parameters.          *           * The option is a:          *<code>org.apache.camel.support.jsse.SSLContextParameters</code> type.          *           * Group: security          */
DECL|method|sslContextParameters ( Object sslContextParameters)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslContextParameters
parameter_list|(
name|Object
name|sslContextParameters
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslContextParameters"
argument_list|,
name|sslContextParameters
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * SSL configuration using a Camel SSLContextParameters object. If          * configured it's applied before the other SSL endpoint parameters.          *           * The option will be converted to a          *<code>org.apache.camel.support.jsse.SSLContextParameters</code> type.          *           * Group: security          */
DECL|method|sslContextParameters ( String sslContextParameters)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslContextParameters
parameter_list|(
name|String
name|sslContextParameters
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslContextParameters"
argument_list|,
name|sslContextParameters
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The list of protocols enabled for SSL connections. TLSv1.2, TLSv1.1          * and TLSv1 are enabled by default.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslEnabledProtocols ( String sslEnabledProtocols)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslEnabledProtocols
parameter_list|(
name|String
name|sslEnabledProtocols
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslEnabledProtocols"
argument_list|,
name|sslEnabledProtocols
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The endpoint identification algorithm to validate server hostname          * using server certificate.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslEndpointAlgorithm ( String sslEndpointAlgorithm)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslEndpointAlgorithm
parameter_list|(
name|String
name|sslEndpointAlgorithm
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslEndpointAlgorithm"
argument_list|,
name|sslEndpointAlgorithm
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The algorithm used by key manager factory for SSL connections.          * Default value is the key manager factory algorithm configured for the          * Java Virtual Machine.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslKeymanagerAlgorithm ( String sslKeymanagerAlgorithm)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslKeymanagerAlgorithm
parameter_list|(
name|String
name|sslKeymanagerAlgorithm
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslKeymanagerAlgorithm"
argument_list|,
name|sslKeymanagerAlgorithm
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The file format of the key store file. This is optional for client.          * Default value is JKS.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslKeystoreType ( String sslKeystoreType)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslKeystoreType
parameter_list|(
name|String
name|sslKeystoreType
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslKeystoreType"
argument_list|,
name|sslKeystoreType
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The SSL protocol used to generate the SSLContext. Default setting is          * TLS, which is fine for most cases. Allowed values in recent JVMs are          * TLS, TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be supported in          * older JVMs, but their usage is discouraged due to known security          * vulnerabilities.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslProtocol (String sslProtocol)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslProtocol
parameter_list|(
name|String
name|sslProtocol
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslProtocol"
argument_list|,
name|sslProtocol
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The name of the security provider used for SSL connections. Default          * value is the default security provider of the JVM.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslProvider (String sslProvider)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslProvider
parameter_list|(
name|String
name|sslProvider
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslProvider"
argument_list|,
name|sslProvider
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The algorithm used by trust manager factory for SSL connections.          * Default value is the trust manager factory algorithm configured for          * the Java Virtual Machine.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslTrustmanagerAlgorithm ( String sslTrustmanagerAlgorithm)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslTrustmanagerAlgorithm
parameter_list|(
name|String
name|sslTrustmanagerAlgorithm
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslTrustmanagerAlgorithm"
argument_list|,
name|sslTrustmanagerAlgorithm
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The file format of the trust store file. Default value is JKS.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslTruststoreType ( String sslTruststoreType)
specifier|default
name|KafkaEndpointConsumerBuilder
name|sslTruststoreType
parameter_list|(
name|String
name|sslTruststoreType
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslTruststoreType"
argument_list|,
name|sslTruststoreType
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * URL of the Confluent Platform schema registry servers to use. The          * format is host1:port1,host2:port2. This is known as          * schema.registry.url in the Confluent Platform documentation. This          * option is only available in the Confluent Platform (not standard          * Apache Kafka).          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: confluent          */
DECL|method|schemaRegistryURL ( String schemaRegistryURL)
specifier|default
name|KafkaEndpointConsumerBuilder
name|schemaRegistryURL
parameter_list|(
name|String
name|schemaRegistryURL
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"schemaRegistryURL"
argument_list|,
name|schemaRegistryURL
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
block|}
comment|/**      * Advanced builder for endpoint consumers for the Kafka component.      */
DECL|interface|AdvancedKafkaEndpointConsumerBuilder
specifier|public
interface|interface
name|AdvancedKafkaEndpointConsumerBuilder
extends|extends
name|EndpointConsumerBuilder
block|{
DECL|method|basic ()
specifier|default
name|KafkaEndpointConsumerBuilder
name|basic
parameter_list|()
block|{
return|return
operator|(
name|KafkaEndpointConsumerBuilder
operator|)
name|this
return|;
block|}
comment|/**          * To let the consumer use a custom ExceptionHandler. Notice if the          * option bridgeErrorHandler is enabled then this option is not in use.          * By default the consumer will deal with exceptions, that will be          * logged at WARN or ERROR level and ignored.          *           * The option is a:<code>org.apache.camel.spi.ExceptionHandler</code>          * type.          *           * Group: consumer (advanced)          */
DECL|method|exceptionHandler ( ExceptionHandler exceptionHandler)
specifier|default
name|AdvancedKafkaEndpointConsumerBuilder
name|exceptionHandler
parameter_list|(
name|ExceptionHandler
name|exceptionHandler
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"exceptionHandler"
argument_list|,
name|exceptionHandler
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * To let the consumer use a custom ExceptionHandler. Notice if the          * option bridgeErrorHandler is enabled then this option is not in use.          * By default the consumer will deal with exceptions, that will be          * logged at WARN or ERROR level and ignored.          *           * The option will be converted to a          *<code>org.apache.camel.spi.ExceptionHandler</code> type.          *           * Group: consumer (advanced)          */
DECL|method|exceptionHandler ( String exceptionHandler)
specifier|default
name|AdvancedKafkaEndpointConsumerBuilder
name|exceptionHandler
parameter_list|(
name|String
name|exceptionHandler
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"exceptionHandler"
argument_list|,
name|exceptionHandler
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets the exchange pattern when the consumer creates an exchange.          *           * The option is a:<code>org.apache.camel.ExchangePattern</code> type.          *           * Group: consumer (advanced)          */
DECL|method|exchangePattern ( ExchangePattern exchangePattern)
specifier|default
name|AdvancedKafkaEndpointConsumerBuilder
name|exchangePattern
parameter_list|(
name|ExchangePattern
name|exchangePattern
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"exchangePattern"
argument_list|,
name|exchangePattern
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets the exchange pattern when the consumer creates an exchange.          *           * The option will be converted to a          *<code>org.apache.camel.ExchangePattern</code> type.          *           * Group: consumer (advanced)          */
DECL|method|exchangePattern ( String exchangePattern)
specifier|default
name|AdvancedKafkaEndpointConsumerBuilder
name|exchangePattern
parameter_list|(
name|String
name|exchangePattern
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"exchangePattern"
argument_list|,
name|exchangePattern
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the endpoint should use basic property binding (Camel 2.x) or          * the newer property binding with additional capabilities.          *           * The option is a:<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|basicPropertyBinding ( boolean basicPropertyBinding)
specifier|default
name|AdvancedKafkaEndpointConsumerBuilder
name|basicPropertyBinding
parameter_list|(
name|boolean
name|basicPropertyBinding
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"basicPropertyBinding"
argument_list|,
name|basicPropertyBinding
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the endpoint should use basic property binding (Camel 2.x) or          * the newer property binding with additional capabilities.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|basicPropertyBinding ( String basicPropertyBinding)
specifier|default
name|AdvancedKafkaEndpointConsumerBuilder
name|basicPropertyBinding
parameter_list|(
name|String
name|basicPropertyBinding
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"basicPropertyBinding"
argument_list|,
name|basicPropertyBinding
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets whether synchronous processing should be strictly used, or Camel          * is allowed to use asynchronous processing (if supported).          *           * The option is a:<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|synchronous ( boolean synchronous)
specifier|default
name|AdvancedKafkaEndpointConsumerBuilder
name|synchronous
parameter_list|(
name|boolean
name|synchronous
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"synchronous"
argument_list|,
name|synchronous
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets whether synchronous processing should be strictly used, or Camel          * is allowed to use asynchronous processing (if supported).          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|synchronous ( String synchronous)
specifier|default
name|AdvancedKafkaEndpointConsumerBuilder
name|synchronous
parameter_list|(
name|String
name|synchronous
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"synchronous"
argument_list|,
name|synchronous
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
block|}
comment|/**      * Builder for endpoint producers for the Kafka component.      */
DECL|interface|KafkaEndpointProducerBuilder
specifier|public
interface|interface
name|KafkaEndpointProducerBuilder
extends|extends
name|EndpointProducerBuilder
block|{
DECL|method|advanced ()
specifier|default
name|AdvancedKafkaEndpointProducerBuilder
name|advanced
parameter_list|()
block|{
return|return
operator|(
name|AdvancedKafkaEndpointProducerBuilder
operator|)
name|this
return|;
block|}
comment|/**          * URL of the Kafka brokers to use. The format is          * host1:port1,host2:port2, and the list can be a subset of brokers or a          * VIP pointing to a subset of brokers. This option is known as          * bootstrap.servers in the Kafka documentation.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: common          */
DECL|method|brokers (String brokers)
specifier|default
name|KafkaEndpointProducerBuilder
name|brokers
parameter_list|(
name|String
name|brokers
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"brokers"
argument_list|,
name|brokers
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The client id is a user-specified string sent in each request to help          * trace calls. It should logically identify the application making the          * request.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: common          */
DECL|method|clientId (String clientId)
specifier|default
name|KafkaEndpointProducerBuilder
name|clientId
parameter_list|(
name|String
name|clientId
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"clientId"
argument_list|,
name|clientId
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * To use a custom HeaderFilterStrategy to filter header to and from          * Camel message.          *           * The option is a:          *<code>org.apache.camel.spi.HeaderFilterStrategy</code> type.          *           * Group: common          */
DECL|method|headerFilterStrategy ( HeaderFilterStrategy headerFilterStrategy)
specifier|default
name|KafkaEndpointProducerBuilder
name|headerFilterStrategy
parameter_list|(
name|HeaderFilterStrategy
name|headerFilterStrategy
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"headerFilterStrategy"
argument_list|,
name|headerFilterStrategy
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * To use a custom HeaderFilterStrategy to filter header to and from          * Camel message.          *           * The option will be converted to a          *<code>org.apache.camel.spi.HeaderFilterStrategy</code> type.          *           * Group: common          */
DECL|method|headerFilterStrategy ( String headerFilterStrategy)
specifier|default
name|KafkaEndpointProducerBuilder
name|headerFilterStrategy
parameter_list|(
name|String
name|headerFilterStrategy
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"headerFilterStrategy"
argument_list|,
name|headerFilterStrategy
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of time in milliseconds to wait when reconnecting          * to a broker that has repeatedly failed to connect. If provided, the          * backoff per host will increase exponentially for each consecutive          * connection failure, up to this maximum. After calculating the backoff          * increase, 20% random jitter is added to avoid connection storms.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: common          */
DECL|method|reconnectBackoffMaxMs ( Integer reconnectBackoffMaxMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|reconnectBackoffMaxMs
parameter_list|(
name|Integer
name|reconnectBackoffMaxMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"reconnectBackoffMaxMs"
argument_list|,
name|reconnectBackoffMaxMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of time in milliseconds to wait when reconnecting          * to a broker that has repeatedly failed to connect. If provided, the          * backoff per host will increase exponentially for each consecutive          * connection failure, up to this maximum. After calculating the backoff          * increase, 20% random jitter is added to avoid connection storms.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: common          */
DECL|method|reconnectBackoffMaxMs ( String reconnectBackoffMaxMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|reconnectBackoffMaxMs
parameter_list|(
name|String
name|reconnectBackoffMaxMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"reconnectBackoffMaxMs"
argument_list|,
name|reconnectBackoffMaxMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * If the option is true, then KafkaProducer will ignore the          * KafkaConstants.TOPIC header setting of the inbound message.          *           * The option is a:<code>boolean</code> type.          *           * Group: producer          */
DECL|method|bridgeEndpoint ( boolean bridgeEndpoint)
specifier|default
name|KafkaEndpointProducerBuilder
name|bridgeEndpoint
parameter_list|(
name|boolean
name|bridgeEndpoint
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"bridgeEndpoint"
argument_list|,
name|bridgeEndpoint
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * If the option is true, then KafkaProducer will ignore the          * KafkaConstants.TOPIC header setting of the inbound message.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: producer          */
DECL|method|bridgeEndpoint ( String bridgeEndpoint)
specifier|default
name|KafkaEndpointProducerBuilder
name|bridgeEndpoint
parameter_list|(
name|String
name|bridgeEndpoint
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"bridgeEndpoint"
argument_list|,
name|bridgeEndpoint
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The total bytes of memory the producer can use to buffer records          * waiting to be sent to the server. If records are sent faster than          * they can be delivered to the server the producer will either block or          * throw an exception based on the preference specified by          * block.on.buffer.full.This setting should correspond roughly to the          * total memory the producer will use, but is not a hard bound since not          * all memory the producer uses is used for buffering. Some additional          * memory will be used for compression (if compression is enabled) as          * well as for maintaining in-flight requests.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|bufferMemorySize ( Integer bufferMemorySize)
specifier|default
name|KafkaEndpointProducerBuilder
name|bufferMemorySize
parameter_list|(
name|Integer
name|bufferMemorySize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"bufferMemorySize"
argument_list|,
name|bufferMemorySize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The total bytes of memory the producer can use to buffer records          * waiting to be sent to the server. If records are sent faster than          * they can be delivered to the server the producer will either block or          * throw an exception based on the preference specified by          * block.on.buffer.full.This setting should correspond roughly to the          * total memory the producer will use, but is not a hard bound since not          * all memory the producer uses is used for buffering. Some additional          * memory will be used for compression (if compression is enabled) as          * well as for maintaining in-flight requests.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|bufferMemorySize ( String bufferMemorySize)
specifier|default
name|KafkaEndpointProducerBuilder
name|bufferMemorySize
parameter_list|(
name|String
name|bufferMemorySize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"bufferMemorySize"
argument_list|,
name|bufferMemorySize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * If the option is true, then KafkaProducer will detect if the message          * is attempted to be sent back to the same topic it may come from, if          * the message was original from a kafka consumer. If the          * KafkaConstants.TOPIC header is the same as the original kafka          * consumer topic, then the header setting is ignored, and the topic of          * the producer endpoint is used. In other words this avoids sending the          * same message back to where it came from. This option is not in use if          * the option bridgeEndpoint is set to true.          *           * The option is a:<code>boolean</code> type.          *           * Group: producer          */
DECL|method|circularTopicDetection ( boolean circularTopicDetection)
specifier|default
name|KafkaEndpointProducerBuilder
name|circularTopicDetection
parameter_list|(
name|boolean
name|circularTopicDetection
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"circularTopicDetection"
argument_list|,
name|circularTopicDetection
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * If the option is true, then KafkaProducer will detect if the message          * is attempted to be sent back to the same topic it may come from, if          * the message was original from a kafka consumer. If the          * KafkaConstants.TOPIC header is the same as the original kafka          * consumer topic, then the header setting is ignored, and the topic of          * the producer endpoint is used. In other words this avoids sending the          * same message back to where it came from. This option is not in use if          * the option bridgeEndpoint is set to true.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: producer          */
DECL|method|circularTopicDetection ( String circularTopicDetection)
specifier|default
name|KafkaEndpointProducerBuilder
name|circularTopicDetection
parameter_list|(
name|String
name|circularTopicDetection
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"circularTopicDetection"
argument_list|,
name|circularTopicDetection
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * This parameter allows you to specify the compression codec for all          * data generated by this producer. Valid values are none, gzip and          * snappy.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: producer          */
DECL|method|compressionCodec ( String compressionCodec)
specifier|default
name|KafkaEndpointProducerBuilder
name|compressionCodec
parameter_list|(
name|String
name|compressionCodec
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"compressionCodec"
argument_list|,
name|compressionCodec
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Close idle connections after the number of milliseconds specified by          * this config.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|connectionMaxIdleMs ( Integer connectionMaxIdleMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|connectionMaxIdleMs
parameter_list|(
name|Integer
name|connectionMaxIdleMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"connectionMaxIdleMs"
argument_list|,
name|connectionMaxIdleMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Close idle connections after the number of milliseconds specified by          * this config.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|connectionMaxIdleMs ( String connectionMaxIdleMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|connectionMaxIdleMs
parameter_list|(
name|String
name|connectionMaxIdleMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"connectionMaxIdleMs"
argument_list|,
name|connectionMaxIdleMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * If set to 'true' the producer will ensure that exactly one copy of          * each message is written in the stream. If 'false', producer retries          * may write duplicates of the retried message in the stream. If set to          * true this option will require max.in.flight.requests.per.connection          * to be set to 1 and retries cannot be zero and additionally acks must          * be set to 'all'.          *           * The option is a:<code>boolean</code> type.          *           * Group: producer          */
DECL|method|enableIdempotence ( boolean enableIdempotence)
specifier|default
name|KafkaEndpointProducerBuilder
name|enableIdempotence
parameter_list|(
name|boolean
name|enableIdempotence
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"enableIdempotence"
argument_list|,
name|enableIdempotence
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * If set to 'true' the producer will ensure that exactly one copy of          * each message is written in the stream. If 'false', producer retries          * may write duplicates of the retried message in the stream. If set to          * true this option will require max.in.flight.requests.per.connection          * to be set to 1 and retries cannot be zero and additionally acks must          * be set to 'all'.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: producer          */
DECL|method|enableIdempotence ( String enableIdempotence)
specifier|default
name|KafkaEndpointProducerBuilder
name|enableIdempotence
parameter_list|(
name|String
name|enableIdempotence
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"enableIdempotence"
argument_list|,
name|enableIdempotence
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets custom KafkaHeaderDeserializer for serialization camel headers          * values to kafka headers values.          *           * The option is a:          *<code>org.apache.camel.component.kafka.serde.KafkaHeaderSerializer</code> type.          *           * Group: producer          */
DECL|method|kafkaHeaderSerializer ( Object kafkaHeaderSerializer)
specifier|default
name|KafkaEndpointProducerBuilder
name|kafkaHeaderSerializer
parameter_list|(
name|Object
name|kafkaHeaderSerializer
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kafkaHeaderSerializer"
argument_list|,
name|kafkaHeaderSerializer
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets custom KafkaHeaderDeserializer for serialization camel headers          * values to kafka headers values.          *           * The option will be converted to a          *<code>org.apache.camel.component.kafka.serde.KafkaHeaderSerializer</code> type.          *           * Group: producer          */
DECL|method|kafkaHeaderSerializer ( String kafkaHeaderSerializer)
specifier|default
name|KafkaEndpointProducerBuilder
name|kafkaHeaderSerializer
parameter_list|(
name|String
name|kafkaHeaderSerializer
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kafkaHeaderSerializer"
argument_list|,
name|kafkaHeaderSerializer
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The record key (or null if no key is specified). If this option has          * been configured then it take precedence over header          * KafkaConstants#KEY.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: producer          */
DECL|method|key (String key)
specifier|default
name|KafkaEndpointProducerBuilder
name|key
parameter_list|(
name|String
name|key
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"key"
argument_list|,
name|key
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The serializer class for keys (defaults to the same as for messages          * if nothing is given).          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: producer          */
DECL|method|keySerializerClass ( String keySerializerClass)
specifier|default
name|KafkaEndpointProducerBuilder
name|keySerializerClass
parameter_list|(
name|String
name|keySerializerClass
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"keySerializerClass"
argument_list|,
name|keySerializerClass
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the producer should be started lazy (on the first message).          * By starting lazy you can use this to allow CamelContext and routes to          * startup in situations where a producer may otherwise fail during          * starting and cause the route to fail being started. By deferring this          * startup to be lazy then the startup failure can be handled during          * routing messages via Camel's routing error handlers. Beware that when          * the first message is processed then creating and starting the          * producer may take a little time and prolong the total processing time          * of the processing.          *           * The option is a:<code>boolean</code> type.          *           * Group: producer          */
DECL|method|lazyStartProducer ( boolean lazyStartProducer)
specifier|default
name|KafkaEndpointProducerBuilder
name|lazyStartProducer
parameter_list|(
name|boolean
name|lazyStartProducer
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"lazyStartProducer"
argument_list|,
name|lazyStartProducer
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the producer should be started lazy (on the first message).          * By starting lazy you can use this to allow CamelContext and routes to          * startup in situations where a producer may otherwise fail during          * starting and cause the route to fail being started. By deferring this          * startup to be lazy then the startup failure can be handled during          * routing messages via Camel's routing error handlers. Beware that when          * the first message is processed then creating and starting the          * producer may take a little time and prolong the total processing time          * of the processing.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: producer          */
DECL|method|lazyStartProducer ( String lazyStartProducer)
specifier|default
name|KafkaEndpointProducerBuilder
name|lazyStartProducer
parameter_list|(
name|String
name|lazyStartProducer
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"lazyStartProducer"
argument_list|,
name|lazyStartProducer
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The producer groups together any records that arrive in between          * request transmissions into a single batched request. Normally this          * occurs only under load when records arrive faster than they can be          * sent out. However in some circumstances the client may want to reduce          * the number of requests even under moderate load. This setting          * accomplishes this by adding a small amount of artificial delaythat          * is, rather than immediately sending out a record the producer will          * wait for up to the given delay to allow other records to be sent so          * that the sends can be batched together. This can be thought of as          * analogous to Nagle's algorithm in TCP. This setting gives the upper          * bound on the delay for batching: once we get batch.size worth of          * records for a partition it will be sent immediately regardless of          * this setting, however if we have fewer than this many bytes          * accumulated for this partition we will 'linger' for the specified          * time waiting for more records to show up. This setting defaults to 0          * (i.e. no delay). Setting linger.ms=5, for example, would have the          * effect of reducing the number of requests sent but would add up to          * 5ms of latency to records sent in the absense of load.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|lingerMs (Integer lingerMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|lingerMs
parameter_list|(
name|Integer
name|lingerMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"lingerMs"
argument_list|,
name|lingerMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The producer groups together any records that arrive in between          * request transmissions into a single batched request. Normally this          * occurs only under load when records arrive faster than they can be          * sent out. However in some circumstances the client may want to reduce          * the number of requests even under moderate load. This setting          * accomplishes this by adding a small amount of artificial delaythat          * is, rather than immediately sending out a record the producer will          * wait for up to the given delay to allow other records to be sent so          * that the sends can be batched together. This can be thought of as          * analogous to Nagle's algorithm in TCP. This setting gives the upper          * bound on the delay for batching: once we get batch.size worth of          * records for a partition it will be sent immediately regardless of          * this setting, however if we have fewer than this many bytes          * accumulated for this partition we will 'linger' for the specified          * time waiting for more records to show up. This setting defaults to 0          * (i.e. no delay). Setting linger.ms=5, for example, would have the          * effect of reducing the number of requests sent but would add up to          * 5ms of latency to records sent in the absense of load.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|lingerMs (String lingerMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|lingerMs
parameter_list|(
name|String
name|lingerMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"lingerMs"
argument_list|,
name|lingerMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The configuration controls how long sending to kafka will block.          * These methods can be blocked for multiple reasons. For e.g: buffer          * full, metadata unavailable.This configuration imposes maximum limit          * on the total time spent in fetching metadata, serialization of key          * and value, partitioning and allocation of buffer memory when doing a          * send(). In case of partitionsFor(), this configuration imposes a          * maximum time threshold on waiting for metadata.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|maxBlockMs (Integer maxBlockMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|maxBlockMs
parameter_list|(
name|Integer
name|maxBlockMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxBlockMs"
argument_list|,
name|maxBlockMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The configuration controls how long sending to kafka will block.          * These methods can be blocked for multiple reasons. For e.g: buffer          * full, metadata unavailable.This configuration imposes maximum limit          * on the total time spent in fetching metadata, serialization of key          * and value, partitioning and allocation of buffer memory when doing a          * send(). In case of partitionsFor(), this configuration imposes a          * maximum time threshold on waiting for metadata.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|maxBlockMs (String maxBlockMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|maxBlockMs
parameter_list|(
name|String
name|maxBlockMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxBlockMs"
argument_list|,
name|maxBlockMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum number of unacknowledged requests the client will send on          * a single connection before blocking. Note that if this setting is set          * to be greater than 1 and there are failed sends, there is a risk of          * message re-ordering due to retries (i.e., if retries are enabled).          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|maxInFlightRequest ( Integer maxInFlightRequest)
specifier|default
name|KafkaEndpointProducerBuilder
name|maxInFlightRequest
parameter_list|(
name|Integer
name|maxInFlightRequest
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxInFlightRequest"
argument_list|,
name|maxInFlightRequest
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum number of unacknowledged requests the client will send on          * a single connection before blocking. Note that if this setting is set          * to be greater than 1 and there are failed sends, there is a risk of          * message re-ordering due to retries (i.e., if retries are enabled).          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|maxInFlightRequest ( String maxInFlightRequest)
specifier|default
name|KafkaEndpointProducerBuilder
name|maxInFlightRequest
parameter_list|(
name|String
name|maxInFlightRequest
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxInFlightRequest"
argument_list|,
name|maxInFlightRequest
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum size of a request. This is also effectively a cap on the          * maximum record size. Note that the server has its own cap on record          * size which may be different from this. This setting will limit the          * number of record batches the producer will send in a single request          * to avoid sending huge requests.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|maxRequestSize ( Integer maxRequestSize)
specifier|default
name|KafkaEndpointProducerBuilder
name|maxRequestSize
parameter_list|(
name|Integer
name|maxRequestSize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxRequestSize"
argument_list|,
name|maxRequestSize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum size of a request. This is also effectively a cap on the          * maximum record size. Note that the server has its own cap on record          * size which may be different from this. This setting will limit the          * number of record batches the producer will send in a single request          * to avoid sending huge requests.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|maxRequestSize ( String maxRequestSize)
specifier|default
name|KafkaEndpointProducerBuilder
name|maxRequestSize
parameter_list|(
name|String
name|maxRequestSize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"maxRequestSize"
argument_list|,
name|maxRequestSize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The period of time in milliseconds after which we force a refresh of          * metadata even if we haven't seen any partition leadership changes to          * proactively discover any new brokers or partitions.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|metadataMaxAgeMs ( Integer metadataMaxAgeMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|metadataMaxAgeMs
parameter_list|(
name|Integer
name|metadataMaxAgeMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"metadataMaxAgeMs"
argument_list|,
name|metadataMaxAgeMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The period of time in milliseconds after which we force a refresh of          * metadata even if we haven't seen any partition leadership changes to          * proactively discover any new brokers or partitions.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|metadataMaxAgeMs ( String metadataMaxAgeMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|metadataMaxAgeMs
parameter_list|(
name|String
name|metadataMaxAgeMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"metadataMaxAgeMs"
argument_list|,
name|metadataMaxAgeMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * A list of classes to use as metrics reporters. Implementing the          * MetricReporter interface allows plugging in classes that will be          * notified of new metric creation. The JmxReporter is always included          * to register JMX statistics.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: producer          */
DECL|method|metricReporters ( String metricReporters)
specifier|default
name|KafkaEndpointProducerBuilder
name|metricReporters
parameter_list|(
name|String
name|metricReporters
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"metricReporters"
argument_list|,
name|metricReporters
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The number of samples maintained to compute metrics.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|metricsSampleWindowMs ( Integer metricsSampleWindowMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|metricsSampleWindowMs
parameter_list|(
name|Integer
name|metricsSampleWindowMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"metricsSampleWindowMs"
argument_list|,
name|metricsSampleWindowMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The number of samples maintained to compute metrics.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|metricsSampleWindowMs ( String metricsSampleWindowMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|metricsSampleWindowMs
parameter_list|(
name|String
name|metricsSampleWindowMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"metricsSampleWindowMs"
argument_list|,
name|metricsSampleWindowMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The number of samples maintained to compute metrics.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|noOfMetricsSample ( Integer noOfMetricsSample)
specifier|default
name|KafkaEndpointProducerBuilder
name|noOfMetricsSample
parameter_list|(
name|Integer
name|noOfMetricsSample
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"noOfMetricsSample"
argument_list|,
name|noOfMetricsSample
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The number of samples maintained to compute metrics.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|noOfMetricsSample ( String noOfMetricsSample)
specifier|default
name|KafkaEndpointProducerBuilder
name|noOfMetricsSample
parameter_list|(
name|String
name|noOfMetricsSample
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"noOfMetricsSample"
argument_list|,
name|noOfMetricsSample
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The partitioner class for partitioning messages amongst sub-topics.          * The default partitioner is based on the hash of the key.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: producer          */
DECL|method|partitioner (String partitioner)
specifier|default
name|KafkaEndpointProducerBuilder
name|partitioner
parameter_list|(
name|String
name|partitioner
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"partitioner"
argument_list|,
name|partitioner
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The partition to which the record will be sent (or null if no          * partition was specified). If this option has been configured then it          * take precedence over header KafkaConstants#PARTITION_KEY.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|partitionKey (Integer partitionKey)
specifier|default
name|KafkaEndpointProducerBuilder
name|partitionKey
parameter_list|(
name|Integer
name|partitionKey
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"partitionKey"
argument_list|,
name|partitionKey
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The partition to which the record will be sent (or null if no          * partition was specified). If this option has been configured then it          * take precedence over header KafkaConstants#PARTITION_KEY.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|partitionKey (String partitionKey)
specifier|default
name|KafkaEndpointProducerBuilder
name|partitionKey
parameter_list|(
name|String
name|partitionKey
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"partitionKey"
argument_list|,
name|partitionKey
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The producer will attempt to batch records together into fewer          * requests whenever multiple records are being sent to the same          * partition. This helps performance on both the client and the server.          * This configuration controls the default batch size in bytes. No          * attempt will be made to batch records larger than this size.Requests          * sent to brokers will contain multiple batches, one for each partition          * with data available to be sent.A small batch size will make batching          * less common and may reduce throughput (a batch size of zero will          * disable batching entirely). A very large batch size may use memory a          * bit more wastefully as we will always allocate a buffer of the          * specified batch size in anticipation of additional records.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|producerBatchSize ( Integer producerBatchSize)
specifier|default
name|KafkaEndpointProducerBuilder
name|producerBatchSize
parameter_list|(
name|Integer
name|producerBatchSize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"producerBatchSize"
argument_list|,
name|producerBatchSize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The producer will attempt to batch records together into fewer          * requests whenever multiple records are being sent to the same          * partition. This helps performance on both the client and the server.          * This configuration controls the default batch size in bytes. No          * attempt will be made to batch records larger than this size.Requests          * sent to brokers will contain multiple batches, one for each partition          * with data available to be sent.A small batch size will make batching          * less common and may reduce throughput (a batch size of zero will          * disable batching entirely). A very large batch size may use memory a          * bit more wastefully as we will always allocate a buffer of the          * specified batch size in anticipation of additional records.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|producerBatchSize ( String producerBatchSize)
specifier|default
name|KafkaEndpointProducerBuilder
name|producerBatchSize
parameter_list|(
name|String
name|producerBatchSize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"producerBatchSize"
argument_list|,
name|producerBatchSize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum number of unsent messages that can be queued up the          * producer when using async mode before either the producer must be          * blocked or data must be dropped.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|queueBufferingMaxMessages ( Integer queueBufferingMaxMessages)
specifier|default
name|KafkaEndpointProducerBuilder
name|queueBufferingMaxMessages
parameter_list|(
name|Integer
name|queueBufferingMaxMessages
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"queueBufferingMaxMessages"
argument_list|,
name|queueBufferingMaxMessages
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum number of unsent messages that can be queued up the          * producer when using async mode before either the producer must be          * blocked or data must be dropped.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|queueBufferingMaxMessages ( String queueBufferingMaxMessages)
specifier|default
name|KafkaEndpointProducerBuilder
name|queueBufferingMaxMessages
parameter_list|(
name|String
name|queueBufferingMaxMessages
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"queueBufferingMaxMessages"
argument_list|,
name|queueBufferingMaxMessages
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The size of the TCP receive buffer (SO_RCVBUF) to use when reading          * data.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|receiveBufferBytes ( Integer receiveBufferBytes)
specifier|default
name|KafkaEndpointProducerBuilder
name|receiveBufferBytes
parameter_list|(
name|Integer
name|receiveBufferBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"receiveBufferBytes"
argument_list|,
name|receiveBufferBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The size of the TCP receive buffer (SO_RCVBUF) to use when reading          * data.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|receiveBufferBytes ( String receiveBufferBytes)
specifier|default
name|KafkaEndpointProducerBuilder
name|receiveBufferBytes
parameter_list|(
name|String
name|receiveBufferBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"receiveBufferBytes"
argument_list|,
name|receiveBufferBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The amount of time to wait before attempting to reconnect to a given          * host. This avoids repeatedly connecting to a host in a tight loop.          * This backoff applies to all requests sent by the consumer to the          * broker.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|reconnectBackoffMs ( Integer reconnectBackoffMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|reconnectBackoffMs
parameter_list|(
name|Integer
name|reconnectBackoffMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"reconnectBackoffMs"
argument_list|,
name|reconnectBackoffMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The amount of time to wait before attempting to reconnect to a given          * host. This avoids repeatedly connecting to a host in a tight loop.          * This backoff applies to all requests sent by the consumer to the          * broker.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|reconnectBackoffMs ( String reconnectBackoffMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|reconnectBackoffMs
parameter_list|(
name|String
name|reconnectBackoffMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"reconnectBackoffMs"
argument_list|,
name|reconnectBackoffMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the producer should store the RecordMetadata results from          * sending to Kafka. The results are stored in a List containing the          * RecordMetadata metadata's. The list is stored on a header with the          * key KafkaConstants#KAFKA_RECORDMETA.          *           * The option is a:<code>boolean</code> type.          *           * Group: producer          */
DECL|method|recordMetadata ( boolean recordMetadata)
specifier|default
name|KafkaEndpointProducerBuilder
name|recordMetadata
parameter_list|(
name|boolean
name|recordMetadata
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"recordMetadata"
argument_list|,
name|recordMetadata
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the producer should store the RecordMetadata results from          * sending to Kafka. The results are stored in a List containing the          * RecordMetadata metadata's. The list is stored on a header with the          * key KafkaConstants#KAFKA_RECORDMETA.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: producer          */
DECL|method|recordMetadata ( String recordMetadata)
specifier|default
name|KafkaEndpointProducerBuilder
name|recordMetadata
parameter_list|(
name|String
name|recordMetadata
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"recordMetadata"
argument_list|,
name|recordMetadata
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The number of acknowledgments the producer requires the leader to          * have received before considering a request complete. This controls          * the durability of records that are sent. The following settings are          * common: acks=0 If set to zero then the producer will not wait for any          * acknowledgment from the server at all. The record will be immediately          * added to the socket buffer and considered sent. No guarantee can be          * made that the server has received the record in this case, and the          * retries configuration will not take effect (as the client won't          * generally know of any failures). The offset given back for each          * record will always be set to -1. acks=1 This will mean the leader          * will write the record to its local log but will respond without          * awaiting full acknowledgement from all followers. In this case should          * the leader fail immediately after acknowledging the record but before          * the followers have replicated it then the record will be lost.          * acks=all This means the leader will wait for the full set of in-sync          * replicas to acknowledge the record. This guarantees that the record          * will not be lost as long as at least one in-sync replica remains          * alive. This is the strongest available guarantee.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: producer          */
DECL|method|requestRequiredAcks ( String requestRequiredAcks)
specifier|default
name|KafkaEndpointProducerBuilder
name|requestRequiredAcks
parameter_list|(
name|String
name|requestRequiredAcks
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"requestRequiredAcks"
argument_list|,
name|requestRequiredAcks
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The amount of time the broker will wait trying to meet the          * request.required.acks requirement before sending back an error to the          * client.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|requestTimeoutMs ( Integer requestTimeoutMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|requestTimeoutMs
parameter_list|(
name|Integer
name|requestTimeoutMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"requestTimeoutMs"
argument_list|,
name|requestTimeoutMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The amount of time the broker will wait trying to meet the          * request.required.acks requirement before sending back an error to the          * client.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|requestTimeoutMs ( String requestTimeoutMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|requestTimeoutMs
parameter_list|(
name|String
name|requestTimeoutMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"requestTimeoutMs"
argument_list|,
name|requestTimeoutMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Setting a value greater than zero will cause the client to resend any          * record whose send fails with a potentially transient error. Note that          * this retry is no different than if the client resent the record upon          * receiving the error. Allowing retries will potentially change the          * ordering of records because if two records are sent to a single          * partition, and the first fails and is retried but the second          * succeeds, then the second record may appear first.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|retries (Integer retries)
specifier|default
name|KafkaEndpointProducerBuilder
name|retries
parameter_list|(
name|Integer
name|retries
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"retries"
argument_list|,
name|retries
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Setting a value greater than zero will cause the client to resend any          * record whose send fails with a potentially transient error. Note that          * this retry is no different than if the client resent the record upon          * receiving the error. Allowing retries will potentially change the          * ordering of records because if two records are sent to a single          * partition, and the first fails and is retried but the second          * succeeds, then the second record may appear first.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|retries (String retries)
specifier|default
name|KafkaEndpointProducerBuilder
name|retries
parameter_list|(
name|String
name|retries
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"retries"
argument_list|,
name|retries
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Before each retry, the producer refreshes the metadata of relevant          * topics to see if a new leader has been elected. Since leader election          * takes a bit of time, this property specifies the amount of time that          * the producer waits before refreshing the metadata.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|retryBackoffMs ( Integer retryBackoffMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|retryBackoffMs
parameter_list|(
name|Integer
name|retryBackoffMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"retryBackoffMs"
argument_list|,
name|retryBackoffMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Before each retry, the producer refreshes the metadata of relevant          * topics to see if a new leader has been elected. Since leader election          * takes a bit of time, this property specifies the amount of time that          * the producer waits before refreshing the metadata.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|retryBackoffMs ( String retryBackoffMs)
specifier|default
name|KafkaEndpointProducerBuilder
name|retryBackoffMs
parameter_list|(
name|String
name|retryBackoffMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"retryBackoffMs"
argument_list|,
name|retryBackoffMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Socket write buffer size.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|sendBufferBytes ( Integer sendBufferBytes)
specifier|default
name|KafkaEndpointProducerBuilder
name|sendBufferBytes
parameter_list|(
name|Integer
name|sendBufferBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sendBufferBytes"
argument_list|,
name|sendBufferBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Socket write buffer size.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|sendBufferBytes ( String sendBufferBytes)
specifier|default
name|KafkaEndpointProducerBuilder
name|sendBufferBytes
parameter_list|(
name|String
name|sendBufferBytes
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sendBufferBytes"
argument_list|,
name|sendBufferBytes
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The serializer class for messages.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: producer          */
DECL|method|serializerClass ( String serializerClass)
specifier|default
name|KafkaEndpointProducerBuilder
name|serializerClass
parameter_list|(
name|String
name|serializerClass
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"serializerClass"
argument_list|,
name|serializerClass
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * To use a custom worker pool for continue routing Exchange after kafka          * server has acknowledge the message that was sent to it from          * KafkaProducer using asynchronous non-blocking processing.          *           * The option is a:<code>java.util.concurrent.ExecutorService</code>          * type.          *           * Group: producer          */
DECL|method|workerPool ( ExecutorService workerPool)
specifier|default
name|KafkaEndpointProducerBuilder
name|workerPool
parameter_list|(
name|ExecutorService
name|workerPool
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"workerPool"
argument_list|,
name|workerPool
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * To use a custom worker pool for continue routing Exchange after kafka          * server has acknowledge the message that was sent to it from          * KafkaProducer using asynchronous non-blocking processing.          *           * The option will be converted to a          *<code>java.util.concurrent.ExecutorService</code> type.          *           * Group: producer          */
DECL|method|workerPool (String workerPool)
specifier|default
name|KafkaEndpointProducerBuilder
name|workerPool
parameter_list|(
name|String
name|workerPool
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"workerPool"
argument_list|,
name|workerPool
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Number of core threads for the worker pool for continue routing          * Exchange after kafka server has acknowledge the message that was sent          * to it from KafkaProducer using asynchronous non-blocking processing.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|workerPoolCoreSize ( Integer workerPoolCoreSize)
specifier|default
name|KafkaEndpointProducerBuilder
name|workerPoolCoreSize
parameter_list|(
name|Integer
name|workerPoolCoreSize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"workerPoolCoreSize"
argument_list|,
name|workerPoolCoreSize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Number of core threads for the worker pool for continue routing          * Exchange after kafka server has acknowledge the message that was sent          * to it from KafkaProducer using asynchronous non-blocking processing.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|workerPoolCoreSize ( String workerPoolCoreSize)
specifier|default
name|KafkaEndpointProducerBuilder
name|workerPoolCoreSize
parameter_list|(
name|String
name|workerPoolCoreSize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"workerPoolCoreSize"
argument_list|,
name|workerPoolCoreSize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Maximum number of threads for the worker pool for continue routing          * Exchange after kafka server has acknowledge the message that was sent          * to it from KafkaProducer using asynchronous non-blocking processing.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: producer          */
DECL|method|workerPoolMaxSize ( Integer workerPoolMaxSize)
specifier|default
name|KafkaEndpointProducerBuilder
name|workerPoolMaxSize
parameter_list|(
name|Integer
name|workerPoolMaxSize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"workerPoolMaxSize"
argument_list|,
name|workerPoolMaxSize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Maximum number of threads for the worker pool for continue routing          * Exchange after kafka server has acknowledge the message that was sent          * to it from KafkaProducer using asynchronous non-blocking processing.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: producer          */
DECL|method|workerPoolMaxSize ( String workerPoolMaxSize)
specifier|default
name|KafkaEndpointProducerBuilder
name|workerPoolMaxSize
parameter_list|(
name|String
name|workerPoolMaxSize
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"workerPoolMaxSize"
argument_list|,
name|workerPoolMaxSize
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets interceptors for producer or consumers. Producer interceptors          * have to be classes implementing          * org.apache.kafka.clients.producer.ProducerInterceptor Consumer          * interceptors have to be classes implementing          * org.apache.kafka.clients.consumer.ConsumerInterceptor Note that if          * you use Producer interceptor on a consumer it will throw a class cast          * exception in runtime.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: monitoring          */
DECL|method|interceptorClasses ( String interceptorClasses)
specifier|default
name|KafkaEndpointProducerBuilder
name|interceptorClasses
parameter_list|(
name|String
name|interceptorClasses
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"interceptorClasses"
argument_list|,
name|interceptorClasses
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread sleep time between refresh attempts.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: security          */
DECL|method|kerberosBeforeReloginMinTime ( Integer kerberosBeforeReloginMinTime)
specifier|default
name|KafkaEndpointProducerBuilder
name|kerberosBeforeReloginMinTime
parameter_list|(
name|Integer
name|kerberosBeforeReloginMinTime
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosBeforeReloginMinTime"
argument_list|,
name|kerberosBeforeReloginMinTime
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread sleep time between refresh attempts.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: security          */
DECL|method|kerberosBeforeReloginMinTime ( String kerberosBeforeReloginMinTime)
specifier|default
name|KafkaEndpointProducerBuilder
name|kerberosBeforeReloginMinTime
parameter_list|(
name|String
name|kerberosBeforeReloginMinTime
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosBeforeReloginMinTime"
argument_list|,
name|kerberosBeforeReloginMinTime
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Kerberos kinit command path. Default is /usr/bin/kinit.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|kerberosInitCmd ( String kerberosInitCmd)
specifier|default
name|KafkaEndpointProducerBuilder
name|kerberosInitCmd
parameter_list|(
name|String
name|kerberosInitCmd
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosInitCmd"
argument_list|,
name|kerberosInitCmd
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * A list of rules for mapping from principal names to short names          * (typically operating system usernames). The rules are evaluated in          * order and the first rule that matches a principal name is used to map          * it to a short name. Any later rules in the list are ignored. By          * default, principal names of the form {username}/{hostname}{REALM} are          * mapped to {username}. For more details on the format please see the          * security authorization and acls documentation.. Multiple values can          * be separated by comma.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|kerberosPrincipalToLocalRules ( String kerberosPrincipalToLocalRules)
specifier|default
name|KafkaEndpointProducerBuilder
name|kerberosPrincipalToLocalRules
parameter_list|(
name|String
name|kerberosPrincipalToLocalRules
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosPrincipalToLocalRules"
argument_list|,
name|kerberosPrincipalToLocalRules
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Percentage of random jitter added to the renewal time.          *           * The option is a:<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewJitter ( Double kerberosRenewJitter)
specifier|default
name|KafkaEndpointProducerBuilder
name|kerberosRenewJitter
parameter_list|(
name|Double
name|kerberosRenewJitter
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewJitter"
argument_list|,
name|kerberosRenewJitter
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Percentage of random jitter added to the renewal time.          *           * The option will be converted to a<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewJitter ( String kerberosRenewJitter)
specifier|default
name|KafkaEndpointProducerBuilder
name|kerberosRenewJitter
parameter_list|(
name|String
name|kerberosRenewJitter
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewJitter"
argument_list|,
name|kerberosRenewJitter
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread will sleep until the specified window factor of time          * from last refresh to ticket's expiry has been reached, at which time          * it will try to renew the ticket.          *           * The option is a:<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewWindowFactor ( Double kerberosRenewWindowFactor)
specifier|default
name|KafkaEndpointProducerBuilder
name|kerberosRenewWindowFactor
parameter_list|(
name|Double
name|kerberosRenewWindowFactor
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewWindowFactor"
argument_list|,
name|kerberosRenewWindowFactor
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread will sleep until the specified window factor of time          * from last refresh to ticket's expiry has been reached, at which time          * it will try to renew the ticket.          *           * The option will be converted to a<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewWindowFactor ( String kerberosRenewWindowFactor)
specifier|default
name|KafkaEndpointProducerBuilder
name|kerberosRenewWindowFactor
parameter_list|(
name|String
name|kerberosRenewWindowFactor
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewWindowFactor"
argument_list|,
name|kerberosRenewWindowFactor
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Expose the kafka sasl.jaas.config parameter Example:          * org.apache.kafka.common.security.plain.PlainLoginModule required          * username=USERNAME password=PASSWORD;.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|saslJaasConfig ( String saslJaasConfig)
specifier|default
name|KafkaEndpointProducerBuilder
name|saslJaasConfig
parameter_list|(
name|String
name|saslJaasConfig
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"saslJaasConfig"
argument_list|,
name|saslJaasConfig
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The Kerberos principal name that Kafka runs as. This can be defined          * either in Kafka's JAAS config or in Kafka's config.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|saslKerberosServiceName ( String saslKerberosServiceName)
specifier|default
name|KafkaEndpointProducerBuilder
name|saslKerberosServiceName
parameter_list|(
name|String
name|saslKerberosServiceName
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"saslKerberosServiceName"
argument_list|,
name|saslKerberosServiceName
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The Simple Authentication and Security Layer (SASL) Mechanism used.          * For the valid values see a href=          * http://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtmlhttp://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|saslMechanism (String saslMechanism)
specifier|default
name|KafkaEndpointProducerBuilder
name|saslMechanism
parameter_list|(
name|String
name|saslMechanism
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"saslMechanism"
argument_list|,
name|saslMechanism
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Protocol used to communicate with brokers. SASL_PLAINTEXT, PLAINTEXT          * and SSL are supported.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|securityProtocol ( String securityProtocol)
specifier|default
name|KafkaEndpointProducerBuilder
name|securityProtocol
parameter_list|(
name|String
name|securityProtocol
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"securityProtocol"
argument_list|,
name|securityProtocol
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * A list of cipher suites. This is a named combination of          * authentication, encryption, MAC and key exchange algorithm used to          * negotiate the security settings for a network connection using TLS or          * SSL network protocol.By default all the available cipher suites are          * supported.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslCipherSuites ( String sslCipherSuites)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslCipherSuites
parameter_list|(
name|String
name|sslCipherSuites
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslCipherSuites"
argument_list|,
name|sslCipherSuites
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * SSL configuration using a Camel SSLContextParameters object. If          * configured it's applied before the other SSL endpoint parameters.          *           * The option is a:          *<code>org.apache.camel.support.jsse.SSLContextParameters</code> type.          *           * Group: security          */
DECL|method|sslContextParameters ( Object sslContextParameters)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslContextParameters
parameter_list|(
name|Object
name|sslContextParameters
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslContextParameters"
argument_list|,
name|sslContextParameters
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * SSL configuration using a Camel SSLContextParameters object. If          * configured it's applied before the other SSL endpoint parameters.          *           * The option will be converted to a          *<code>org.apache.camel.support.jsse.SSLContextParameters</code> type.          *           * Group: security          */
DECL|method|sslContextParameters ( String sslContextParameters)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslContextParameters
parameter_list|(
name|String
name|sslContextParameters
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslContextParameters"
argument_list|,
name|sslContextParameters
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The list of protocols enabled for SSL connections. TLSv1.2, TLSv1.1          * and TLSv1 are enabled by default.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslEnabledProtocols ( String sslEnabledProtocols)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslEnabledProtocols
parameter_list|(
name|String
name|sslEnabledProtocols
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslEnabledProtocols"
argument_list|,
name|sslEnabledProtocols
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The endpoint identification algorithm to validate server hostname          * using server certificate.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslEndpointAlgorithm ( String sslEndpointAlgorithm)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslEndpointAlgorithm
parameter_list|(
name|String
name|sslEndpointAlgorithm
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslEndpointAlgorithm"
argument_list|,
name|sslEndpointAlgorithm
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The algorithm used by key manager factory for SSL connections.          * Default value is the key manager factory algorithm configured for the          * Java Virtual Machine.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslKeymanagerAlgorithm ( String sslKeymanagerAlgorithm)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslKeymanagerAlgorithm
parameter_list|(
name|String
name|sslKeymanagerAlgorithm
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslKeymanagerAlgorithm"
argument_list|,
name|sslKeymanagerAlgorithm
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The password of the private key in the key store file. This is          * optional for client.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslKeyPassword ( String sslKeyPassword)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslKeyPassword
parameter_list|(
name|String
name|sslKeyPassword
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslKeyPassword"
argument_list|,
name|sslKeyPassword
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The location of the key store file. This is optional for client and          * can be used for two-way authentication for client.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslKeystoreLocation ( String sslKeystoreLocation)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslKeystoreLocation
parameter_list|(
name|String
name|sslKeystoreLocation
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslKeystoreLocation"
argument_list|,
name|sslKeystoreLocation
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The store password for the key store file.This is optional for client          * and only needed if ssl.keystore.location is configured.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslKeystorePassword ( String sslKeystorePassword)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslKeystorePassword
parameter_list|(
name|String
name|sslKeystorePassword
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslKeystorePassword"
argument_list|,
name|sslKeystorePassword
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The file format of the key store file. This is optional for client.          * Default value is JKS.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslKeystoreType ( String sslKeystoreType)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslKeystoreType
parameter_list|(
name|String
name|sslKeystoreType
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslKeystoreType"
argument_list|,
name|sslKeystoreType
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The SSL protocol used to generate the SSLContext. Default setting is          * TLS, which is fine for most cases. Allowed values in recent JVMs are          * TLS, TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be supported in          * older JVMs, but their usage is discouraged due to known security          * vulnerabilities.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslProtocol (String sslProtocol)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslProtocol
parameter_list|(
name|String
name|sslProtocol
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslProtocol"
argument_list|,
name|sslProtocol
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The name of the security provider used for SSL connections. Default          * value is the default security provider of the JVM.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslProvider (String sslProvider)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslProvider
parameter_list|(
name|String
name|sslProvider
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslProvider"
argument_list|,
name|sslProvider
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The algorithm used by trust manager factory for SSL connections.          * Default value is the trust manager factory algorithm configured for          * the Java Virtual Machine.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslTrustmanagerAlgorithm ( String sslTrustmanagerAlgorithm)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslTrustmanagerAlgorithm
parameter_list|(
name|String
name|sslTrustmanagerAlgorithm
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslTrustmanagerAlgorithm"
argument_list|,
name|sslTrustmanagerAlgorithm
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The file format of the trust store file. Default value is JKS.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslTruststoreType ( String sslTruststoreType)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslTruststoreType
parameter_list|(
name|String
name|sslTruststoreType
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslTruststoreType"
argument_list|,
name|sslTruststoreType
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * URL of the Confluent Platform schema registry servers to use. The          * format is host1:port1,host2:port2. This is known as          * schema.registry.url in the Confluent Platform documentation. This          * option is only available in the Confluent Platform (not standard          * Apache Kafka).          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: confluent          */
DECL|method|schemaRegistryURL ( String schemaRegistryURL)
specifier|default
name|KafkaEndpointProducerBuilder
name|schemaRegistryURL
parameter_list|(
name|String
name|schemaRegistryURL
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"schemaRegistryURL"
argument_list|,
name|schemaRegistryURL
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The location of the trust store file.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslTruststoreLocation ( String sslTruststoreLocation)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslTruststoreLocation
parameter_list|(
name|String
name|sslTruststoreLocation
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslTruststoreLocation"
argument_list|,
name|sslTruststoreLocation
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The password for the trust store file.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslTruststorePassword ( String sslTruststorePassword)
specifier|default
name|KafkaEndpointProducerBuilder
name|sslTruststorePassword
parameter_list|(
name|String
name|sslTruststorePassword
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslTruststorePassword"
argument_list|,
name|sslTruststorePassword
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
block|}
comment|/**      * Advanced builder for endpoint producers for the Kafka component.      */
DECL|interface|AdvancedKafkaEndpointProducerBuilder
specifier|public
interface|interface
name|AdvancedKafkaEndpointProducerBuilder
extends|extends
name|EndpointProducerBuilder
block|{
DECL|method|basic ()
specifier|default
name|KafkaEndpointProducerBuilder
name|basic
parameter_list|()
block|{
return|return
operator|(
name|KafkaEndpointProducerBuilder
operator|)
name|this
return|;
block|}
comment|/**          * Whether the endpoint should use basic property binding (Camel 2.x) or          * the newer property binding with additional capabilities.          *           * The option is a:<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|basicPropertyBinding ( boolean basicPropertyBinding)
specifier|default
name|AdvancedKafkaEndpointProducerBuilder
name|basicPropertyBinding
parameter_list|(
name|boolean
name|basicPropertyBinding
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"basicPropertyBinding"
argument_list|,
name|basicPropertyBinding
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the endpoint should use basic property binding (Camel 2.x) or          * the newer property binding with additional capabilities.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|basicPropertyBinding ( String basicPropertyBinding)
specifier|default
name|AdvancedKafkaEndpointProducerBuilder
name|basicPropertyBinding
parameter_list|(
name|String
name|basicPropertyBinding
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"basicPropertyBinding"
argument_list|,
name|basicPropertyBinding
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets whether synchronous processing should be strictly used, or Camel          * is allowed to use asynchronous processing (if supported).          *           * The option is a:<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|synchronous ( boolean synchronous)
specifier|default
name|AdvancedKafkaEndpointProducerBuilder
name|synchronous
parameter_list|(
name|boolean
name|synchronous
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"synchronous"
argument_list|,
name|synchronous
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets whether synchronous processing should be strictly used, or Camel          * is allowed to use asynchronous processing (if supported).          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|synchronous ( String synchronous)
specifier|default
name|AdvancedKafkaEndpointProducerBuilder
name|synchronous
parameter_list|(
name|String
name|synchronous
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"synchronous"
argument_list|,
name|synchronous
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
block|}
comment|/**      * Builder for endpoint for the Kafka component.      */
DECL|interface|KafkaEndpointBuilder
specifier|public
interface|interface
name|KafkaEndpointBuilder
extends|extends
name|KafkaEndpointConsumerBuilder
extends|,
name|KafkaEndpointProducerBuilder
block|{
DECL|method|advanced ()
specifier|default
name|AdvancedKafkaEndpointBuilder
name|advanced
parameter_list|()
block|{
return|return
operator|(
name|AdvancedKafkaEndpointBuilder
operator|)
name|this
return|;
block|}
comment|/**          * URL of the Kafka brokers to use. The format is          * host1:port1,host2:port2, and the list can be a subset of brokers or a          * VIP pointing to a subset of brokers. This option is known as          * bootstrap.servers in the Kafka documentation.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: common          */
DECL|method|brokers (String brokers)
specifier|default
name|KafkaEndpointBuilder
name|brokers
parameter_list|(
name|String
name|brokers
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"brokers"
argument_list|,
name|brokers
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The client id is a user-specified string sent in each request to help          * trace calls. It should logically identify the application making the          * request.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: common          */
DECL|method|clientId (String clientId)
specifier|default
name|KafkaEndpointBuilder
name|clientId
parameter_list|(
name|String
name|clientId
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"clientId"
argument_list|,
name|clientId
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * To use a custom HeaderFilterStrategy to filter header to and from          * Camel message.          *           * The option is a:          *<code>org.apache.camel.spi.HeaderFilterStrategy</code> type.          *           * Group: common          */
DECL|method|headerFilterStrategy ( HeaderFilterStrategy headerFilterStrategy)
specifier|default
name|KafkaEndpointBuilder
name|headerFilterStrategy
parameter_list|(
name|HeaderFilterStrategy
name|headerFilterStrategy
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"headerFilterStrategy"
argument_list|,
name|headerFilterStrategy
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * To use a custom HeaderFilterStrategy to filter header to and from          * Camel message.          *           * The option will be converted to a          *<code>org.apache.camel.spi.HeaderFilterStrategy</code> type.          *           * Group: common          */
DECL|method|headerFilterStrategy ( String headerFilterStrategy)
specifier|default
name|KafkaEndpointBuilder
name|headerFilterStrategy
parameter_list|(
name|String
name|headerFilterStrategy
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"headerFilterStrategy"
argument_list|,
name|headerFilterStrategy
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of time in milliseconds to wait when reconnecting          * to a broker that has repeatedly failed to connect. If provided, the          * backoff per host will increase exponentially for each consecutive          * connection failure, up to this maximum. After calculating the backoff          * increase, 20% random jitter is added to avoid connection storms.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: common          */
DECL|method|reconnectBackoffMaxMs ( Integer reconnectBackoffMaxMs)
specifier|default
name|KafkaEndpointBuilder
name|reconnectBackoffMaxMs
parameter_list|(
name|Integer
name|reconnectBackoffMaxMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"reconnectBackoffMaxMs"
argument_list|,
name|reconnectBackoffMaxMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The maximum amount of time in milliseconds to wait when reconnecting          * to a broker that has repeatedly failed to connect. If provided, the          * backoff per host will increase exponentially for each consecutive          * connection failure, up to this maximum. After calculating the backoff          * increase, 20% random jitter is added to avoid connection storms.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: common          */
DECL|method|reconnectBackoffMaxMs ( String reconnectBackoffMaxMs)
specifier|default
name|KafkaEndpointBuilder
name|reconnectBackoffMaxMs
parameter_list|(
name|String
name|reconnectBackoffMaxMs
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"reconnectBackoffMaxMs"
argument_list|,
name|reconnectBackoffMaxMs
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets interceptors for producer or consumers. Producer interceptors          * have to be classes implementing          * org.apache.kafka.clients.producer.ProducerInterceptor Consumer          * interceptors have to be classes implementing          * org.apache.kafka.clients.consumer.ConsumerInterceptor Note that if          * you use Producer interceptor on a consumer it will throw a class cast          * exception in runtime.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: monitoring          */
DECL|method|interceptorClasses ( String interceptorClasses)
specifier|default
name|KafkaEndpointBuilder
name|interceptorClasses
parameter_list|(
name|String
name|interceptorClasses
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"interceptorClasses"
argument_list|,
name|interceptorClasses
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread sleep time between refresh attempts.          *           * The option is a:<code>java.lang.Integer</code> type.          *           * Group: security          */
DECL|method|kerberosBeforeReloginMinTime ( Integer kerberosBeforeReloginMinTime)
specifier|default
name|KafkaEndpointBuilder
name|kerberosBeforeReloginMinTime
parameter_list|(
name|Integer
name|kerberosBeforeReloginMinTime
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosBeforeReloginMinTime"
argument_list|,
name|kerberosBeforeReloginMinTime
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread sleep time between refresh attempts.          *           * The option will be converted to a<code>java.lang.Integer</code>          * type.          *           * Group: security          */
DECL|method|kerberosBeforeReloginMinTime ( String kerberosBeforeReloginMinTime)
specifier|default
name|KafkaEndpointBuilder
name|kerberosBeforeReloginMinTime
parameter_list|(
name|String
name|kerberosBeforeReloginMinTime
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosBeforeReloginMinTime"
argument_list|,
name|kerberosBeforeReloginMinTime
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Kerberos kinit command path. Default is /usr/bin/kinit.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|kerberosInitCmd (String kerberosInitCmd)
specifier|default
name|KafkaEndpointBuilder
name|kerberosInitCmd
parameter_list|(
name|String
name|kerberosInitCmd
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosInitCmd"
argument_list|,
name|kerberosInitCmd
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * A list of rules for mapping from principal names to short names          * (typically operating system usernames). The rules are evaluated in          * order and the first rule that matches a principal name is used to map          * it to a short name. Any later rules in the list are ignored. By          * default, principal names of the form {username}/{hostname}{REALM} are          * mapped to {username}. For more details on the format please see the          * security authorization and acls documentation.. Multiple values can          * be separated by comma.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|kerberosPrincipalToLocalRules ( String kerberosPrincipalToLocalRules)
specifier|default
name|KafkaEndpointBuilder
name|kerberosPrincipalToLocalRules
parameter_list|(
name|String
name|kerberosPrincipalToLocalRules
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosPrincipalToLocalRules"
argument_list|,
name|kerberosPrincipalToLocalRules
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Percentage of random jitter added to the renewal time.          *           * The option is a:<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewJitter ( Double kerberosRenewJitter)
specifier|default
name|KafkaEndpointBuilder
name|kerberosRenewJitter
parameter_list|(
name|Double
name|kerberosRenewJitter
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewJitter"
argument_list|,
name|kerberosRenewJitter
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Percentage of random jitter added to the renewal time.          *           * The option will be converted to a<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewJitter ( String kerberosRenewJitter)
specifier|default
name|KafkaEndpointBuilder
name|kerberosRenewJitter
parameter_list|(
name|String
name|kerberosRenewJitter
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewJitter"
argument_list|,
name|kerberosRenewJitter
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread will sleep until the specified window factor of time          * from last refresh to ticket's expiry has been reached, at which time          * it will try to renew the ticket.          *           * The option is a:<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewWindowFactor ( Double kerberosRenewWindowFactor)
specifier|default
name|KafkaEndpointBuilder
name|kerberosRenewWindowFactor
parameter_list|(
name|Double
name|kerberosRenewWindowFactor
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewWindowFactor"
argument_list|,
name|kerberosRenewWindowFactor
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Login thread will sleep until the specified window factor of time          * from last refresh to ticket's expiry has been reached, at which time          * it will try to renew the ticket.          *           * The option will be converted to a<code>java.lang.Double</code> type.          *           * Group: security          */
DECL|method|kerberosRenewWindowFactor ( String kerberosRenewWindowFactor)
specifier|default
name|KafkaEndpointBuilder
name|kerberosRenewWindowFactor
parameter_list|(
name|String
name|kerberosRenewWindowFactor
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"kerberosRenewWindowFactor"
argument_list|,
name|kerberosRenewWindowFactor
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Expose the kafka sasl.jaas.config parameter Example:          * org.apache.kafka.common.security.plain.PlainLoginModule required          * username=USERNAME password=PASSWORD;.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|saslJaasConfig (String saslJaasConfig)
specifier|default
name|KafkaEndpointBuilder
name|saslJaasConfig
parameter_list|(
name|String
name|saslJaasConfig
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"saslJaasConfig"
argument_list|,
name|saslJaasConfig
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The Kerberos principal name that Kafka runs as. This can be defined          * either in Kafka's JAAS config or in Kafka's config.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|saslKerberosServiceName ( String saslKerberosServiceName)
specifier|default
name|KafkaEndpointBuilder
name|saslKerberosServiceName
parameter_list|(
name|String
name|saslKerberosServiceName
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"saslKerberosServiceName"
argument_list|,
name|saslKerberosServiceName
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The Simple Authentication and Security Layer (SASL) Mechanism used.          * For the valid values see a href=          * http://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtmlhttp://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|saslMechanism (String saslMechanism)
specifier|default
name|KafkaEndpointBuilder
name|saslMechanism
parameter_list|(
name|String
name|saslMechanism
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"saslMechanism"
argument_list|,
name|saslMechanism
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Protocol used to communicate with brokers. SASL_PLAINTEXT, PLAINTEXT          * and SSL are supported.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|securityProtocol (String securityProtocol)
specifier|default
name|KafkaEndpointBuilder
name|securityProtocol
parameter_list|(
name|String
name|securityProtocol
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"securityProtocol"
argument_list|,
name|securityProtocol
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * A list of cipher suites. This is a named combination of          * authentication, encryption, MAC and key exchange algorithm used to          * negotiate the security settings for a network connection using TLS or          * SSL network protocol.By default all the available cipher suites are          * supported.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslCipherSuites (String sslCipherSuites)
specifier|default
name|KafkaEndpointBuilder
name|sslCipherSuites
parameter_list|(
name|String
name|sslCipherSuites
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslCipherSuites"
argument_list|,
name|sslCipherSuites
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * SSL configuration using a Camel SSLContextParameters object. If          * configured it's applied before the other SSL endpoint parameters.          *           * The option is a:          *<code>org.apache.camel.support.jsse.SSLContextParameters</code> type.          *           * Group: security          */
DECL|method|sslContextParameters ( Object sslContextParameters)
specifier|default
name|KafkaEndpointBuilder
name|sslContextParameters
parameter_list|(
name|Object
name|sslContextParameters
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslContextParameters"
argument_list|,
name|sslContextParameters
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * SSL configuration using a Camel SSLContextParameters object. If          * configured it's applied before the other SSL endpoint parameters.          *           * The option will be converted to a          *<code>org.apache.camel.support.jsse.SSLContextParameters</code> type.          *           * Group: security          */
DECL|method|sslContextParameters ( String sslContextParameters)
specifier|default
name|KafkaEndpointBuilder
name|sslContextParameters
parameter_list|(
name|String
name|sslContextParameters
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslContextParameters"
argument_list|,
name|sslContextParameters
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The list of protocols enabled for SSL connections. TLSv1.2, TLSv1.1          * and TLSv1 are enabled by default.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslEnabledProtocols ( String sslEnabledProtocols)
specifier|default
name|KafkaEndpointBuilder
name|sslEnabledProtocols
parameter_list|(
name|String
name|sslEnabledProtocols
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslEnabledProtocols"
argument_list|,
name|sslEnabledProtocols
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The endpoint identification algorithm to validate server hostname          * using server certificate.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslEndpointAlgorithm ( String sslEndpointAlgorithm)
specifier|default
name|KafkaEndpointBuilder
name|sslEndpointAlgorithm
parameter_list|(
name|String
name|sslEndpointAlgorithm
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslEndpointAlgorithm"
argument_list|,
name|sslEndpointAlgorithm
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The algorithm used by key manager factory for SSL connections.          * Default value is the key manager factory algorithm configured for the          * Java Virtual Machine.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslKeymanagerAlgorithm ( String sslKeymanagerAlgorithm)
specifier|default
name|KafkaEndpointBuilder
name|sslKeymanagerAlgorithm
parameter_list|(
name|String
name|sslKeymanagerAlgorithm
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslKeymanagerAlgorithm"
argument_list|,
name|sslKeymanagerAlgorithm
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The file format of the key store file. This is optional for client.          * Default value is JKS.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslKeystoreType (String sslKeystoreType)
specifier|default
name|KafkaEndpointBuilder
name|sslKeystoreType
parameter_list|(
name|String
name|sslKeystoreType
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslKeystoreType"
argument_list|,
name|sslKeystoreType
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The SSL protocol used to generate the SSLContext. Default setting is          * TLS, which is fine for most cases. Allowed values in recent JVMs are          * TLS, TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be supported in          * older JVMs, but their usage is discouraged due to known security          * vulnerabilities.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslProtocol (String sslProtocol)
specifier|default
name|KafkaEndpointBuilder
name|sslProtocol
parameter_list|(
name|String
name|sslProtocol
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslProtocol"
argument_list|,
name|sslProtocol
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The name of the security provider used for SSL connections. Default          * value is the default security provider of the JVM.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslProvider (String sslProvider)
specifier|default
name|KafkaEndpointBuilder
name|sslProvider
parameter_list|(
name|String
name|sslProvider
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslProvider"
argument_list|,
name|sslProvider
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The algorithm used by trust manager factory for SSL connections.          * Default value is the trust manager factory algorithm configured for          * the Java Virtual Machine.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslTrustmanagerAlgorithm ( String sslTrustmanagerAlgorithm)
specifier|default
name|KafkaEndpointBuilder
name|sslTrustmanagerAlgorithm
parameter_list|(
name|String
name|sslTrustmanagerAlgorithm
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslTrustmanagerAlgorithm"
argument_list|,
name|sslTrustmanagerAlgorithm
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * The file format of the trust store file. Default value is JKS.          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: security          */
DECL|method|sslTruststoreType (String sslTruststoreType)
specifier|default
name|KafkaEndpointBuilder
name|sslTruststoreType
parameter_list|(
name|String
name|sslTruststoreType
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"sslTruststoreType"
argument_list|,
name|sslTruststoreType
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * URL of the Confluent Platform schema registry servers to use. The          * format is host1:port1,host2:port2. This is known as          * schema.registry.url in the Confluent Platform documentation. This          * option is only available in the Confluent Platform (not standard          * Apache Kafka).          *           * The option is a:<code>java.lang.String</code> type.          *           * Group: confluent          */
DECL|method|schemaRegistryURL (String schemaRegistryURL)
specifier|default
name|KafkaEndpointBuilder
name|schemaRegistryURL
parameter_list|(
name|String
name|schemaRegistryURL
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"schemaRegistryURL"
argument_list|,
name|schemaRegistryURL
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
block|}
comment|/**      * Advanced builder for endpoint for the Kafka component.      */
DECL|interface|AdvancedKafkaEndpointBuilder
specifier|public
interface|interface
name|AdvancedKafkaEndpointBuilder
extends|extends
name|AdvancedKafkaEndpointConsumerBuilder
extends|,
name|AdvancedKafkaEndpointProducerBuilder
block|{
DECL|method|basic ()
specifier|default
name|KafkaEndpointBuilder
name|basic
parameter_list|()
block|{
return|return
operator|(
name|KafkaEndpointBuilder
operator|)
name|this
return|;
block|}
comment|/**          * Whether the endpoint should use basic property binding (Camel 2.x) or          * the newer property binding with additional capabilities.          *           * The option is a:<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|basicPropertyBinding ( boolean basicPropertyBinding)
specifier|default
name|AdvancedKafkaEndpointBuilder
name|basicPropertyBinding
parameter_list|(
name|boolean
name|basicPropertyBinding
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"basicPropertyBinding"
argument_list|,
name|basicPropertyBinding
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Whether the endpoint should use basic property binding (Camel 2.x) or          * the newer property binding with additional capabilities.          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|basicPropertyBinding ( String basicPropertyBinding)
specifier|default
name|AdvancedKafkaEndpointBuilder
name|basicPropertyBinding
parameter_list|(
name|String
name|basicPropertyBinding
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"basicPropertyBinding"
argument_list|,
name|basicPropertyBinding
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets whether synchronous processing should be strictly used, or Camel          * is allowed to use asynchronous processing (if supported).          *           * The option is a:<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|synchronous (boolean synchronous)
specifier|default
name|AdvancedKafkaEndpointBuilder
name|synchronous
parameter_list|(
name|boolean
name|synchronous
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"synchronous"
argument_list|,
name|synchronous
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**          * Sets whether synchronous processing should be strictly used, or Camel          * is allowed to use asynchronous processing (if supported).          *           * The option will be converted to a<code>boolean</code> type.          *           * Group: advanced          */
DECL|method|synchronous (String synchronous)
specifier|default
name|AdvancedKafkaEndpointBuilder
name|synchronous
parameter_list|(
name|String
name|synchronous
parameter_list|)
block|{
name|doSetProperty
argument_list|(
literal|"synchronous"
argument_list|,
name|synchronous
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
block|}
comment|/**      * Kafka (camel-kafka)      * The kafka component allows messages to be sent to (or consumed from)      * Apache Kafka brokers.      *       * Category: messaging      * Available as of version: 2.13      * Maven coordinates: org.apache.camel:camel-kafka      *       * Syntax:<code>kafka:topic</code>      *       * Path parameter: topic (required)      * Name of the topic to use. On the consumer you can use comma to separate      * multiple topics. A producer can only send a message to a single topic.      */
DECL|method|kafka (String path)
specifier|default
name|KafkaEndpointBuilder
name|kafka
parameter_list|(
name|String
name|path
parameter_list|)
block|{
class|class
name|KafkaEndpointBuilderImpl
extends|extends
name|AbstractEndpointBuilder
implements|implements
name|KafkaEndpointBuilder
implements|,
name|AdvancedKafkaEndpointBuilder
block|{
specifier|public
name|KafkaEndpointBuilderImpl
parameter_list|(
name|String
name|path
parameter_list|)
block|{
name|super
argument_list|(
literal|"kafka"
argument_list|,
name|path
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|new
name|KafkaEndpointBuilderImpl
argument_list|(
name|path
argument_list|)
return|;
block|}
block|}
end_interface

end_unit

